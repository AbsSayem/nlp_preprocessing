{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"nlp_preprocessing.ipynb","provenance":[],"collapsed_sections":["cmnbK60QOGiQ"],"authorship_tag":"ABX9TyO0f3UDpqaPEtaDcxjsenuk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"cmnbK60QOGiQ"},"source":["# **Tokenization**"]},{"cell_type":"markdown","metadata":{"id":"WNUl_HIxM38B"},"source":["**Using Python \"split()\" function**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K2NsZEWgLFzL","executionInfo":{"status":"ok","timestamp":1634982096250,"user_tz":-360,"elapsed":691,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"a387c30f-bf07-441e-a1d4-2cbe587a3a06"},"source":["# Spliting by words\n","text = \"\"\"This is text for testing preprocessing steps for nlp. For nlp, preprocessing steps are very much important. Preprocessing steps improve the accuracy in percentage(%). If you don't follow preprocessing steps, you can't get the desired result. Preprocessing steps varies from purpose to purpose.\"\"\"\n","print(text.split())     # This will by default split the text by words"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['This', 'is', 'text', 'for', 'testing', 'preprocessing', 'steps', 'for', 'nlp.', 'For', 'nlp,', 'preprocessing', 'steps', 'are', 'very', 'much', 'important.', 'Preprocessing', 'steps', 'improve', 'the', 'accuracy', 'in', 'percentage(%).', 'If', 'you', \"don't\", 'follow', 'preprocessing', 'steps,', 'you', \"can't\", 'get', 'the', 'desired', 'result.', 'Preprocessing', 'steps', 'varies', 'from', 'purpose', 'to', 'purpose.']\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sV5JYiNTNi-I","executionInfo":{"status":"ok","timestamp":1634982096898,"user_tz":-360,"elapsed":14,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"5307c3fd-3cc0-4bd8-ae06-e764a6a43571"},"source":["# Spliting by words\n","text = \"\"\"This is text for testing preprocessing steps for nlp. For nlp, preprocessing steps are very much important. Preprocessing steps improve the accuracy in percentage(%). If you don't follow preprocessing steps, you can't get the desired result. Preprocessing steps varies from purpose to purpose.\"\"\"\n","text.split('.')     # This will  split the text by dots(.)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['This is text for testing preprocessing steps for nlp',\n"," ' For nlp, preprocessing steps are very much important',\n"," ' Preprocessing steps improve the accuracy in percentage(%)',\n"," \" If you don't follow preprocessing steps, you can't get the desired result\",\n"," ' Preprocessing steps varies from purpose to purpose',\n"," '']"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"iu-rsRrON1jA"},"source":["**Using Regular Expression**"]},{"cell_type":"code","metadata":{"id":"QCbmAcRLOBM7"},"source":["# Importing regular expression\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yarTZkddOt0_","executionInfo":{"status":"ok","timestamp":1634982096899,"user_tz":-360,"elapsed":12,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"a85c88f8-a066-4aaa-c5ba-0ca0c03d9d2d"},"source":["# Word Tokenization\n","text = \"\"\"This is text for testing preprocessing steps for nlp. For nlp, preprocessing steps are very much important. Preprocessing steps improve the accuracy in percentage(%). If you don't follow preprocessing steps, you can't get the desired result. Preprocessing steps varies from purpose to purpose.\"\"\"\n","tokens = re.findall(\"[\\w']+\", text)\n","print(tokens)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['This', 'is', 'text', 'for', 'testing', 'preprocessing', 'steps', 'for', 'nlp', 'For', 'nlp', 'preprocessing', 'steps', 'are', 'very', 'much', 'important', 'Preprocessing', 'steps', 'improve', 'the', 'accuracy', 'in', 'percentage', 'If', 'you', \"don't\", 'follow', 'preprocessing', 'steps', 'you', \"can't\", 'get', 'the', 'desired', 'result', 'Preprocessing', 'steps', 'varies', 'from', 'purpose', 'to', 'purpose']\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-QzJ_e8FUhQh","executionInfo":{"status":"ok","timestamp":1634982096900,"user_tz":-360,"elapsed":11,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"d768131d-c682-45ea-e750-a9a6a8edcde2"},"source":["# Sentence Tokenization\n","# To perform sentence tokenization we can use \"re.split()\" method. We can pass multiple separators in it.\n","text = \"\"\"This is text for testing preprocessing steps for nlp. For nlp, preprocessing steps are very much important. Preprocessing steps improve the accuracy in percentage(%). If you don't follow preprocessing steps, you can't get the desired result. Preprocessing steps varies from purpose to purpose.\"\"\"\n","sentences = re.compile('[.!?]').split(text)\n","sentences"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['This is text for testing preprocessing steps for nlp',\n"," ' For nlp, preprocessing steps are very much important',\n"," ' Preprocessing steps improve the accuracy in percentage(%)',\n"," \" If you don't follow preprocessing steps, you can't get the desired result\",\n"," ' Preprocessing steps varies from purpose to purpose',\n"," '']"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"BdAOcnJRAA-4"},"source":["**Usinjg NLTK**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RqbTMBu1AFRx","executionInfo":{"status":"ok","timestamp":1634982100270,"user_tz":-360,"elapsed":3379,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"b7958413-96be-4f16-bc91-f4077a77303e"},"source":["# Download and Import NLTK\n","!pip install nltk\n","import nltk\n","nltk.download('punkt')    #punkt is a nltk library tool for tokenizing text documents."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JvJ4JqedBT7E","executionInfo":{"status":"ok","timestamp":1634982100272,"user_tz":-360,"elapsed":26,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"356d4283-5de9-499b-d08e-0d49bbf80801"},"source":["# Word tokenization\n","from nltk.tokenize import word_tokenize\n","text = \"\"\"This is text for testing preprocessing steps for nlp. For nlp, preprocessing steps are very much important. Preprocessing steps improve the accuracy in percentage(%). If you don't follow preprocessing steps, you can't get the desired result. Preprocessing steps varies from purpose to purpose.\"\"\"\n","print(word_tokenize(text))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['This', 'is', 'text', 'for', 'testing', 'preprocessing', 'steps', 'for', 'nlp', '.', 'For', 'nlp', ',', 'preprocessing', 'steps', 'are', 'very', 'much', 'important', '.', 'Preprocessing', 'steps', 'improve', 'the', 'accuracy', 'in', 'percentage', '(', '%', ')', '.', 'If', 'you', 'do', \"n't\", 'follow', 'preprocessing', 'steps', ',', 'you', 'ca', \"n't\", 'get', 'the', 'desired', 'result', '.', 'Preprocessing', 'steps', 'varies', 'from', 'purpose', 'to', 'purpose', '.']\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"onTGaTPnE92W","executionInfo":{"status":"ok","timestamp":1634982100275,"user_tz":-360,"elapsed":20,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"5b861bc8-d204-4ef6-937c-75b1a93ebcb6"},"source":["# Word tokenization\n","from nltk.tokenize import sent_tokenize\n","text = \"\"\"This is text for testing preprocessing steps for nlp. For nlp, preprocessing steps are very much important. Preprocessing steps improve the accuracy in percentage(%). If you don't follow preprocessing steps, you can't get the desired result. Preprocessing steps varies from purpose to purpose.\"\"\"\n","sent_tokenize(text)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['This is text for testing preprocessing steps for nlp.',\n"," 'For nlp, preprocessing steps are very much important.',\n"," 'Preprocessing steps improve the accuracy in percentage(%).',\n"," \"If you don't follow preprocessing steps, you can't get the desired result.\",\n"," 'Preprocessing steps varies from purpose to purpose.']"]},"metadata":{},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"NZYfz_YoS_kD"},"source":["**Using spaCy**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qDJuhQZTTFDK","executionInfo":{"status":"ok","timestamp":1634982103310,"user_tz":-360,"elapsed":3051,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"14dd803e-c04c-48eb-ca16-6776d9ae4e8d"},"source":["#Download and Install spacy\n","#!pip install -U pip setuptools wheel\n","!pip install  spacy\n","#python -m spacy download en_core_web_sm    #[didn't work]"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.8.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.6.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7uep7IuNWezl","executionInfo":{"status":"ok","timestamp":1634982107688,"user_tz":-360,"elapsed":4394,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"f1d2e20f-c01b-4080-bbad-99c323aeeed1"},"source":["import spacy\n","spacy.cli.download(\"en_core_web_sm\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GCA76wBJXFjg","executionInfo":{"status":"ok","timestamp":1636785502889,"user_tz":-360,"elapsed":442,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"d1989830-fc5e-480b-c027-08a7aa3f7e49"},"source":["# Word Tokenization\n","from spacy.lang.en import English\n","# Load english tokenizer\n","nlp = English()     # nlp is used to craeate documents with linguistic annotations.\n","text = \"\"\"This is text for testing preprocessing steps for nlp. For nlp, preprocessing steps are very much important. Preprocessing steps improve the accuracy in percentage(%). If you don't follow preprocessing steps, you can't get the desired result. Preprocessing steps varies from purpose to purpose.\"\"\"\n","doc = nlp(text)   # Creates the document using nlp\n","print(doc)\n","\n","# tokenizing the document and create a list of tokens\n","tokens = []\n","for token in doc:\n","  tokens.append(token.text)\n","print(tokens)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["This is text for testing preprocessing steps for nlp. For nlp, preprocessing steps are very much important. Preprocessing steps improve the accuracy in percentage(%). If you don't follow preprocessing steps, you can't get the desired result. Preprocessing steps varies from purpose to purpose.\n","['This', 'is', 'text', 'for', 'testing', 'preprocessing', 'steps', 'for', 'nlp', '.', 'For', 'nlp', ',', 'preprocessing', 'steps', 'are', 'very', 'much', 'important', '.', 'Preprocessing', 'steps', 'improve', 'the', 'accuracy', 'in', 'percentage(%', ')', '.', 'If', 'you', 'do', \"n't\", 'follow', 'preprocessing', 'steps', ',', 'you', 'ca', \"n't\", 'get', 'the', 'desired', 'result', '.', 'Preprocessing', 'steps', 'varies', 'from', 'purpose', 'to', 'purpose', '.']\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9uejLFb2KxhB","executionInfo":{"status":"ok","timestamp":1634988979845,"user_tz":-360,"elapsed":393,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"4d118fac-969c-464c-992a-4730b1a2535c"},"source":["# Sentence Tokenization\n","from spacy.lang.en import English\n","\n","# Load english tokenizer\n","nlp = English()     # nlp is used to craeate documents with linguistic annotations.\n","\n","# Create the pipeline 'sentencizer' component\n","sbd = nlp.create_pipe('sentencizer')\n","\n","# Add the component to the pipeline\n","nlp.add_pipe(sbd)\n","\n","text = \"\"\"This is text for testing preprocessing steps for nlp. For nlp, preprocessing steps are very much important. Preprocessing steps improve the accuracy in percentage(%). If you don't follow preprocessing steps, you can't get the desired result. Preprocessing steps varies from purpose to purpose.\"\"\"\n","doc = nlp(text)   # Creates the document using nlp\n","\n","# create list of sentence tokens\n","sents_list = []\n","for sent in doc.sents:\n","    sents_list.append(sent.text)\n","    #sents_list = sent.string.strip()\n","sents_list"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['This is text for testing preprocessing steps for nlp.',\n"," 'For nlp, preprocessing steps are very much important.',\n"," 'Preprocessing steps improve the accuracy in percentage(%).',\n"," \"If you don't follow preprocessing steps, you can't get the desired result.\",\n"," 'Preprocessing steps varies from purpose to purpose.']"]},"metadata":{},"execution_count":46}]},{"cell_type":"markdown","metadata":{"id":"BAx9UnQxQcjH"},"source":["**Using Keras**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q-ycqYr_QgUr","executionInfo":{"status":"ok","timestamp":1634982111111,"user_tz":-360,"elapsed":2782,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"e3c2a91f-6b2c-4937-f024-78b5749c8bcc"},"source":["# Install Keras\n","!pip install keras"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.6.0)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ftx3CKkvRRF0","executionInfo":{"status":"ok","timestamp":1634982111117,"user_tz":-360,"elapsed":34,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"b40e292a-ac3f-4a30-9a67-5adad3abb643"},"source":["# Word Tokenization\n","from keras.preprocessing.text import text_to_word_sequence\n","text = \"\"\"This is text for testing preprocessing steps for nlp. For nlp, preprocessing steps are very much important. Preprocessing steps improve the accuracy in percentage(%). If you don't follow preprocessing steps, you can't get the desired result. Preprocessing steps varies from purpose to purpose.\"\"\"\n","word_tokens = text_to_word_sequence(text)\n","print(word_tokens)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['this', 'is', 'text', 'for', 'testing', 'preprocessing', 'steps', 'for', 'nlp', 'for', 'nlp', 'preprocessing', 'steps', 'are', 'very', 'much', 'important', 'preprocessing', 'steps', 'improve', 'the', 'accuracy', 'in', 'percentage', 'if', 'you', \"don't\", 'follow', 'preprocessing', 'steps', 'you', \"can't\", 'get', 'the', 'desired', 'result', 'preprocessing', 'steps', 'varies', 'from', 'purpose', 'to', 'purpose']\n"]}]},{"cell_type":"markdown","metadata":{"id":"d0I4u6QzUDyV"},"source":["**Using Gensim**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"90-y50bNSK_k","executionInfo":{"status":"ok","timestamp":1634982114312,"user_tz":-360,"elapsed":3219,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"79d1f3b9-f78b-4727-c295-efc105fd0f0c"},"source":["!pip install gensim"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rhE1smP-VYwh","executionInfo":{"status":"ok","timestamp":1634982114315,"user_tz":-360,"elapsed":61,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"3d635013-0994-4cc9-f3fe-a5eb7273df0a"},"source":["# Word tokenization\n","from gensim.utils import tokenize\n","text = \"\"\"This is text for testing preprocessing steps for nlp. For nlp, preprocessing steps are very much important. Preprocessing steps improve the accuracy in percentage(%). If you don't follow preprocessing steps, you can't get the desired result. Preprocessing steps varies from purpose to purpose.\"\"\"\n","tokens = list(tokenize(text))\n","print(tokens)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['This', 'is', 'text', 'for', 'testing', 'preprocessing', 'steps', 'for', 'nlp', 'For', 'nlp', 'preprocessing', 'steps', 'are', 'very', 'much', 'important', 'Preprocessing', 'steps', 'improve', 'the', 'accuracy', 'in', 'percentage', 'If', 'you', 'don', 't', 'follow', 'preprocessing', 'steps', 'you', 'can', 't', 'get', 'the', 'desired', 'result', 'Preprocessing', 'steps', 'varies', 'from', 'purpose', 'to', 'purpose']\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TwHAGEamW0_Q","executionInfo":{"status":"ok","timestamp":1634982114318,"user_tz":-360,"elapsed":54,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"9934ceb3-906b-477b-e7eb-52db72db0b06"},"source":["# Sentence Tokenization\n","from gensim.summarization.textcleaner import split_sentences\n","text = \"\"\"This is text for testing preprocessing steps for nlp. For nlp, preprocessing steps are very much important. Preprocessing steps improve the accuracy in percentage(%). If you don't follow preprocessing steps, you can't get the desired result. Preprocessing steps varies from purpose to purpose.\"\"\"\n","sentences = split_sentences(text)\n","sentences"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['This is text for testing preprocessing steps for nlp.',\n"," 'For nlp, preprocessing steps are very much important.',\n"," 'Preprocessing steps improve the accuracy in percentage(%).',\n"," \"If you don't follow preprocessing steps, you can't get the desired result.\",\n"," 'Preprocessing steps varies from purpose to purpose.']"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"VmDh6Q47BtHe"},"source":["# **Stemming**"]},{"cell_type":"markdown","metadata":{"id":"uSLeEZTFM6Zu"},"source":["**Using NLTK**"]},{"cell_type":"code","metadata":{"id":"gwfh-4r1Byvr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636966467214,"user_tz":-360,"elapsed":1796,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"ca923b66-9dc2-456d-c16d-15f7c384fdf9"},"source":["# Download and Install NLTK       # For first time use\n","import nltk       # import the nltk package\n","nltk.download(\"punkt\")     # call the nltk downloader"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"bMmbnOH2i-5R"},"source":["# Porter Stemmer\n","from nltk.stem import PorterStemmer\n","from nltk.stem import LancasterStemmer\n","from nltk.stem import SnowballStemmer\n","#create an object of class PorterStemmer\n","porter = PorterStemmer()\n","lancaster = LancasterStemmer()\n","snowball = SnowballStemmer('english')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2PZp-3VKjglA","executionInfo":{"status":"ok","timestamp":1636966467217,"user_tz":-360,"elapsed":18,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"411f5dd2-7642-4668-8a02-1197f3ea76ba"},"source":["# Stemming words using Porter Stemmer\n","print(\"trouble: \" , porter.stem(\"trouble\"))\n","print(\"troubling: \", porter.stem(\"troubling\"))\n","print(\"troubled: \", porter.stem(\"troubled\"))\n","print(\"-----------------------------\")\n","print(\"likes: \" , porter.stem(\"likes\"))\n","print(\"liked: \", porter.stem(\"liked\"))\n","print(\"like: \", porter.stem(\"like\"))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["trouble:  troubl\n","troubling:  troubl\n","troubled:  troubl\n","-----------------------------\n","likes:  like\n","liked:  like\n","like:  like\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SaF9Hchxqz7k","executionInfo":{"status":"ok","timestamp":1636966467218,"user_tz":-360,"elapsed":15,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"13b086f6-bcbe-41cf-8dd0-e7699d0d160b"},"source":["# Stemming words using Lancaster Stemmer\n","print(\"trouble: \" , lancaster.stem(\"trouble\"))\n","print(\"troubling: \", lancaster.stem(\"troubling\"))\n","print(\"troubled: \", lancaster.stem(\"troubled\"))\n","print(\"-----------------------------\")\n","print(\"likes: \" , lancaster.stem(\"likes\"))\n","print(\"liked: \", lancaster.stem(\"liked\"))\n","print(\"like: \", lancaster.stem(\"like\"))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["trouble:  troubl\n","troubling:  troubl\n","troubled:  troubl\n","-----------------------------\n","likes:  lik\n","liked:  lik\n","like:  lik\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0qMroT3Ir6pE","executionInfo":{"status":"ok","timestamp":1636966467964,"user_tz":-360,"elapsed":758,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"123cc082-f01c-45bf-e398-92a24e28d14e"},"source":["# Stemming words using Snowball Stemmer\n","print(\"trouble: \" , snowball.stem(\"trouble\"))\n","print(\"troubling: \", snowball.stem(\"troubling\"))\n","print(\"troubled: \", snowball.stem(\"troubled\"))\n","print(\"-----------------------------\")\n","print(\"likes: \" , snowball.stem(\"likes\"))\n","print(\"liked: \", snowball.stem(\"liked\"))\n","print(\"like: \", snowball.stem(\"like\"))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["trouble:  troubl\n","troubling:  troubl\n","troubled:  troubl\n","-----------------------------\n","likes:  like\n","liked:  like\n","like:  like\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BGa-1I1Ol1Rn","executionInfo":{"status":"ok","timestamp":1636966467966,"user_tz":-360,"elapsed":56,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"9a2f9975-e114-474f-d5a9-c223cb28b95f"},"source":["# Stemming list of words using Porter Stemmer\n","words = [\"basically\", \"anything\", \"most\", \"nouns\", \"always\", \"funny\", \"being\", \"running\", \"adverbs\", \"question\", \"against\", \"when\", \"let's\", \"preposition\"]\n","for word in words:\n","  print(\"{}: {}\".format(word, porter.stem(word)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["basically: basic\n","anything: anyth\n","most: most\n","nouns: noun\n","always: alway\n","funny: funni\n","being: be\n","running: run\n","adverbs: adverb\n","question: question\n","against: against\n","when: when\n","let's: let'\n","preposition: preposit\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WR_oEZr0rOPP","executionInfo":{"status":"ok","timestamp":1636966467967,"user_tz":-360,"elapsed":52,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"04c9f939-988d-4ab5-b178-285fbe167e8b"},"source":["# Stemming list of words using Lancaster Stemmer\n","words = [\"basically\", \"anything\", \"most\", \"nouns\", \"always\", \"funny\", \"being\", \"running\", \"adverbs\", \"question\", \"against\", \"when\", \"let's\", \"preposition\"]\n","for word in words:\n","  print(\"{} - {} \".format(word, lancaster.stem(word)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["basically - bas \n","anything - anyth \n","most - most \n","nouns - noun \n","always - alway \n","funny - funny \n","being - being \n","running - run \n","adverbs - adverb \n","question - quest \n","against - against \n","when - when \n","let's - let's \n","preposition - preposit \n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3tT7XbCCsN8z","executionInfo":{"status":"ok","timestamp":1636966467968,"user_tz":-360,"elapsed":48,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"28037f90-7c4b-46a1-a84c-597045491d1f"},"source":["# Stemming list of words using Snowball Stemmer\n","words = [\"basically\", \"anything\", \"most\", \"nouns\", \"always\", \"funny\", \"being\", \"running\", \"adverbs\", \"question\", \"against\", \"when\", \"let's\", \"preposition\"]\n","for word in words:\n","  print(\"{} = {}\".format(word, snowball.stem(word)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["basically = basic\n","anything = anyth\n","most = most\n","nouns = noun\n","always = alway\n","funny = funni\n","being = be\n","running = run\n","adverbs = adverb\n","question = question\n","against = against\n","when = when\n","let's = let\n","preposition = preposit\n"]}]},{"cell_type":"markdown","metadata":{"id":"2gx99RGg85h7"},"source":["**Stemming Sentence**"]},{"cell_type":"code","metadata":{"id":"IALGFgZg9-hc"},"source":["# Stemming sentence using Porter Stemmer\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","def pstemSentence(sentence):\n","    token_words=word_tokenize(sentence)\n","    stem_sentence=[]\n","    for word in token_words:\n","        stem_sentence.append(porter.stem(word))   # Stems the words and append them to the list one by one\n","        stem_sentence.append(\" \")     # add a white space after every stemmed word\n","    return \"\".join(stem_sentence)     # Join all of them together"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AjwomZB5N5Jd"},"source":["# Stemming sentence using Lancaster Stemmer\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","def lstemSentence(sentence):\n","    token_words=word_tokenize(sentence)\n","    stem_sentence=[]\n","    for word in token_words:\n","        stem_sentence.append(lancaster.stem(word))   # Stems the words and append them to the list one by one\n","        stem_sentence.append(\" \")     # add a white space after every stemmed word\n","    return \"\".join(stem_sentence)     # Join all of them together"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hl70x_6KN7rb"},"source":["# Stemming sentence using Snowball Stemmer\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","def sstemSentence(sentence):\n","    token_words=word_tokenize(sentence)\n","    stem_sentence=[]\n","    for word in token_words:\n","        stem_sentence.append(snowball.stem(word))   # Stems the words and append them to the list one by one\n","        stem_sentence.append(\" \")     # add a white space after every stemmed word\n","    return \"\".join(stem_sentence)     # Join all of them together"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3XKlCwy6AE8J","executionInfo":{"status":"ok","timestamp":1636966467973,"user_tz":-360,"elapsed":39,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"daf6a176-1689-47f2-c608-cc8352882026"},"source":["# Stemming sentence \n","sentence = \"A quick brown fox jumps over the lazy dog.\"\n","print(\"    Original Sentence: \", sentence)\n","# Applying stemSentence module\n","x = pstemSentence(sentence)\n","print(\"      Porter Stemmed: \", x)\n","y = lstemSentence(sentence)\n","print(\"Lancaster Stemmed: \", y)\n","z = sstemSentence(sentence)\n","print(\" Snowball Stemmed: \", z)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["    Original Sentence:  A quick brown fox jumps over the lazy dog.\n","      Porter Stemmed:  A quick brown fox jump over the lazi dog . \n","Lancaster Stemmed:  a quick brown fox jump ov the lazy dog . \n"," Snowball Stemmed:  a quick brown fox jump over the lazi dog . \n"]}]},{"cell_type":"code","metadata":{"id":"7JT1tPk2OChr"},"source":["# Define a Stemmer Module\n","# To stemm the words of a sentence we need to tokenize the sentence by words.\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n","#from nltk.stem import LancasterStemmer\n","#from nltk.stem import SnowballStemmer\n","porter = PorterStemmer()\n","lancaster = LancasterStemmer()\n","snowball = SnowballStemmer('english')\n","def SentenceStemmer(sentence, stemmer):\n","  tokens = word_tokenize(sentence)\n","  stemmed_sentence=[]\n","  for token in tokens:\n","    if stemmer ==1:\n","      stemmed_sentence.append(porter.stem(token))\n","    elif stemmer ==2:\n","      stemmed_sentence.append(lancaster.stem(token))\n","    elif stemmer ==3:\n","      stemmed_sentence.append(snowball.stem(token))\n","    stemmed_sentence.append(\" \")\n","  return \"\".join(stemmed_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8z1zrjzgjcxM","executionInfo":{"status":"ok","timestamp":1636966467976,"user_tz":-360,"elapsed":32,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"8ab3ee75-a60c-4135-c9ed-66efcfa1a29a"},"source":["# Stemming sentence \n","sentence = \"A quick brown fox jumps over the lazy dog.\"\n","print(sentence)\n","x = SentenceStemmer(sentence, 1)\n","print(x)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["A quick brown fox jumps over the lazy dog.\n","A quick brown fox jump over the lazi dog . \n"]}]},{"cell_type":"markdown","metadata":{"id":"0Te5EpNQA4RX"},"source":["**Modularize the Algorithms**"]},{"cell_type":"code","metadata":{"id":"yDkWHLM-Zwb5"},"source":["# Define a Stemmer Module\n","# To stemm the words of a sentence we need to tokenize the sentence by words.\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n","#from nltk.stem import LancasterStemmer\n","#from nltk.stem import SnowballStemmer\n","porter = PorterStemmer()\n","lancaster = LancasterStemmer()\n","snowball = SnowballStemmer('english')\n","def SentenceStemmer(sentence, stemmer):\n","  tokens = word_tokenize(sentence)\n","  stemmed_sentence = []\n","  choice = stemmer\n","  if choice == 1:\n","    for word in tokens:\n","      stemmed_sentence.append(porter.stem(word))\n","      stemmed_sentence.append(\" \")\n","  if choice == 2:\n","    for word in tokens:\n","      stemmed_sentence.append(lancaster.stem(word))\n","      stemmed_sentence.append(\" \")\n","  if choice == 3:\n","    for word in tokens:\n","      stemmed_sentence.append(snowball.stem(word))\n","      stemmed_sentence.append(\" \")\n","  return \"\".join(stemmed_sentence)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SfCR7F628yEl","executionInfo":{"status":"ok","timestamp":1636966525310,"user_tz":-360,"elapsed":3663,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"43636e89-7701-4033-a83f-845c0755467b"},"source":["# Stemming sentence \n","sentence = \"A quick brown fox jumps over the lazy dog.\"\n","# Applying the stemSentence module\n","print(\"Chose the Stemmer: \\n\\t1 => Porter Stemmer\\n\\t2 => Lancaster Stemmer\\n\\t3 => Snowball Stemmer\\n=======================\")\n","stemmer = int(input(\"You Chose: \"))     # In Python, when use input - you need to typecast the input type.\n","print(\"=======================\")\n","print(\"    Original Sentence: \", sentence)\n","print(\"Stemmed Sentence: \", SentenceStemmer(sentence, stemmer))\n","#print(\"Stemmed Sentence: \", x)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Chose the Stemmer: \n","\t1 => Porter Stemmer\n","\t2 => Lancaster Stemmer\n","\t3 => Snowball Stemmer\n","=======================\n","You Chose: 1\n","=======================\n","    Original Sentence:  A quick brown fox jumps over the lazy dog.\n","Stemmed Sentence:  A quick brown fox jump over the lazi dog . \n"]}]},{"cell_type":"markdown","metadata":{"id":"KFZxDB0iAsS0"},"source":["**Stemming a document**\n","1. Take a document as the input\n","2. Read the document line by line\n","3. Tokenize the line by words\n","4. Stem the words"]},{"cell_type":"code","metadata":{"id":"xKEKnbKvAyw3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636967041900,"user_tz":-360,"elapsed":431,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"fad4fbb4-bced-4774-ab88-2a9800e60008"},"source":["file = open(\"nlp_preprocessing.txt\")\n","#file.read()\n","sentence_list = file.readlines()\n","sentence_list"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['One of the first things required for natural language processing (NLP) tasks is a corpus.\\n',\n"," 'In linguistics and NLP, corpus refers to a collection of texts.\\n',\n"," 'Such collections may be formed of a single language of texts, or can span multiple languages - there are numerous reasons for which multilingual corpora, the plural of corpus, may be useful.\\n',\n"," 'Corpora may also consist of themed texts, like- historical, Biblical, etc.\\n',\n"," 'Corpora are generally solely used for statistical linguistic analysis and hypothesis testing.\\n',\n"," 'The good thing is that the internet is filled with text, and in many cases this text is collected and well oganized, even if it requires some finessing into a more usable, precisely-defined format.\\n',\n"," 'Wikipedia, in particular, is a rich source of well-organized textual data.\\n',\n"," \"It's also a vast collection of knowledge, and the unhampered mind can dream up all sorts of uses for just such a body of text.\\n\",\n"," 'What we will do here is build a corpus from the set of English Wikipedia articles, which is freely and conveniently available online.\\n',\n"," 'In order to easily build a text corpus void of the Wikipedia article markup, we will use gensim, a topic modeling library for Python.\\n',\n"," 'A Wikipedia dump file is also required for this procedure, quite obviously.\\n',\n"," 'The latest such files can be found here.\\n',\n"," 'I wrote a simple Python script to build the corpus by stripping all Wikipedia markup from the articles, using gensim.\\n',\n"," 'You can read up on the WikiCorpus class here.\\n',\n"," 'A second script then checks the corpus text file we just built.\\n',\n"," 'Now, keep in mind that this large Wikipedia dump file then resulted in a very large corpus file.\\n',\n"," 'Given its enormous size, you may have dificulty reading the full file into memory at one time.\\n',\n"," \"This script, then, starts by reading 50 lines - which equates to 50 full articles - from the text file and outputting them to the terminal, after which you can press a key to output another 50, or type 'STOP' to quit.\\n\",\n"," 'If you do stop, the script then proceeds to load the entire file into memory.\\n',\n"," 'Which could be a problem for you.\\n',\n"," 'You can, however, verify the text by batches of lines, in order to satisfy your curiousity that something good happened as a result of running the first script.\\n',\n"," \"If you are planning on working on such a large text file, you may need some workarounds for its large size in comparison to your machine's memory.\"]"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pA-siqOlM477","executionInfo":{"status":"ok","timestamp":1636967048688,"user_tz":-360,"elapsed":675,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"9d6e184a-d8b1-49a5-aa1d-4b7afc6a37ef"},"source":["# Remove white space and new line.\n","converted_list = []\n","for elements in sentence_list:\n","  converted_list.append(elements.strip())   # strip() - removes the white-spaces and new-line from a list\n","converted_list"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['One of the first things required for natural language processing (NLP) tasks is a corpus.',\n"," 'In linguistics and NLP, corpus refers to a collection of texts.',\n"," 'Such collections may be formed of a single language of texts, or can span multiple languages - there are numerous reasons for which multilingual corpora, the plural of corpus, may be useful.',\n"," 'Corpora may also consist of themed texts, like- historical, Biblical, etc.',\n"," 'Corpora are generally solely used for statistical linguistic analysis and hypothesis testing.',\n"," 'The good thing is that the internet is filled with text, and in many cases this text is collected and well oganized, even if it requires some finessing into a more usable, precisely-defined format.',\n"," 'Wikipedia, in particular, is a rich source of well-organized textual data.',\n"," \"It's also a vast collection of knowledge, and the unhampered mind can dream up all sorts of uses for just such a body of text.\",\n"," 'What we will do here is build a corpus from the set of English Wikipedia articles, which is freely and conveniently available online.',\n"," 'In order to easily build a text corpus void of the Wikipedia article markup, we will use gensim, a topic modeling library for Python.',\n"," 'A Wikipedia dump file is also required for this procedure, quite obviously.',\n"," 'The latest such files can be found here.',\n"," 'I wrote a simple Python script to build the corpus by stripping all Wikipedia markup from the articles, using gensim.',\n"," 'You can read up on the WikiCorpus class here.',\n"," 'A second script then checks the corpus text file we just built.',\n"," 'Now, keep in mind that this large Wikipedia dump file then resulted in a very large corpus file.',\n"," 'Given its enormous size, you may have dificulty reading the full file into memory at one time.',\n"," \"This script, then, starts by reading 50 lines - which equates to 50 full articles - from the text file and outputting them to the terminal, after which you can press a key to output another 50, or type 'STOP' to quit.\",\n"," 'If you do stop, the script then proceeds to load the entire file into memory.',\n"," 'Which could be a problem for you.',\n"," 'You can, however, verify the text by batches of lines, in order to satisfy your curiousity that something good happened as a result of running the first script.',\n"," \"If you are planning on working on such a large text file, you may need some workarounds for its large size in comparison to your machine's memory.\"]"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2w1m75R2J3UN","executionInfo":{"status":"ok","timestamp":1636967059000,"user_tz":-360,"elapsed":4723,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"4180b29a-fbaf-40c8-9565-26747976571b"},"source":["\n","# Applying the stemSentence module\n","print(\"Chose the Stemmer: \\n\\t1 => Porter Stemmer\\n\\t2 => Lancaster Stemmer\\n\\t3 => Snowball Stemmer\\n=======================\")\n","stemmer = int(input(\"You Chose: \"))     # In Python, when use input - you need to typecast the input type.\n","print(\"=======================\")\n","for sentences in converted_list:\n","  print(\"\\nOriginal Sentence: \", sentences)\n","  print(\" Stemmed Sentence: \", SentenceStemmer(sentences, stemmer))\n","#print(\"Stemmed Sentence: \", x)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Chose the Stemmer: \n","\t1 => Porter Stemmer\n","\t2 => Lancaster Stemmer\n","\t3 => Snowball Stemmer\n","=======================\n","You Chose: 1\n","=======================\n","\n","Original Sentence:  One of the first things required for natural language processing (NLP) tasks is a corpus.\n"," Stemmed Sentence:  one of the first thing requir for natur languag process ( nlp ) task is a corpu . \n","\n","Original Sentence:  In linguistics and NLP, corpus refers to a collection of texts.\n"," Stemmed Sentence:  In linguist and nlp , corpu refer to a collect of text . \n","\n","Original Sentence:  Such collections may be formed of a single language of texts, or can span multiple languages - there are numerous reasons for which multilingual corpora, the plural of corpus, may be useful.\n"," Stemmed Sentence:  such collect may be form of a singl languag of text , or can span multipl languag - there are numer reason for which multilingu corpora , the plural of corpu , may be use . \n","\n","Original Sentence:  Corpora may also consist of themed texts, like- historical, Biblical, etc.\n"," Stemmed Sentence:  corpora may also consist of theme text , like- histor , biblic , etc . \n","\n","Original Sentence:  Corpora are generally solely used for statistical linguistic analysis and hypothesis testing.\n"," Stemmed Sentence:  corpora are gener sole use for statist linguist analysi and hypothesi test . \n","\n","Original Sentence:  The good thing is that the internet is filled with text, and in many cases this text is collected and well oganized, even if it requires some finessing into a more usable, precisely-defined format.\n"," Stemmed Sentence:  the good thing is that the internet is fill with text , and in mani case thi text is collect and well ogan , even if it requir some finess into a more usabl , precisely-defin format . \n","\n","Original Sentence:  Wikipedia, in particular, is a rich source of well-organized textual data.\n"," Stemmed Sentence:  wikipedia , in particular , is a rich sourc of well-organ textual data . \n","\n","Original Sentence:  It's also a vast collection of knowledge, and the unhampered mind can dream up all sorts of uses for just such a body of text.\n"," Stemmed Sentence:  It 's also a vast collect of knowledg , and the unhamp mind can dream up all sort of use for just such a bodi of text . \n","\n","Original Sentence:  What we will do here is build a corpus from the set of English Wikipedia articles, which is freely and conveniently available online.\n"," Stemmed Sentence:  what we will do here is build a corpu from the set of english wikipedia articl , which is freeli and conveni avail onlin . \n","\n","Original Sentence:  In order to easily build a text corpus void of the Wikipedia article markup, we will use gensim, a topic modeling library for Python.\n"," Stemmed Sentence:  In order to easili build a text corpu void of the wikipedia articl markup , we will use gensim , a topic model librari for python . \n","\n","Original Sentence:  A Wikipedia dump file is also required for this procedure, quite obviously.\n"," Stemmed Sentence:  A wikipedia dump file is also requir for thi procedur , quit obvious . \n","\n","Original Sentence:  The latest such files can be found here.\n"," Stemmed Sentence:  the latest such file can be found here . \n","\n","Original Sentence:  I wrote a simple Python script to build the corpus by stripping all Wikipedia markup from the articles, using gensim.\n"," Stemmed Sentence:  I wrote a simpl python script to build the corpu by strip all wikipedia markup from the articl , use gensim . \n","\n","Original Sentence:  You can read up on the WikiCorpus class here.\n"," Stemmed Sentence:  you can read up on the wikicorpu class here . \n","\n","Original Sentence:  A second script then checks the corpus text file we just built.\n"," Stemmed Sentence:  A second script then check the corpu text file we just built . \n","\n","Original Sentence:  Now, keep in mind that this large Wikipedia dump file then resulted in a very large corpus file.\n"," Stemmed Sentence:  now , keep in mind that thi larg wikipedia dump file then result in a veri larg corpu file . \n","\n","Original Sentence:  Given its enormous size, you may have dificulty reading the full file into memory at one time.\n"," Stemmed Sentence:  given it enorm size , you may have dificulti read the full file into memori at one time . \n","\n","Original Sentence:  This script, then, starts by reading 50 lines - which equates to 50 full articles - from the text file and outputting them to the terminal, after which you can press a key to output another 50, or type 'STOP' to quit.\n"," Stemmed Sentence:  thi script , then , start by read 50 line - which equat to 50 full articl - from the text file and output them to the termin , after which you can press a key to output anoth 50 , or type 'stop ' to quit . \n","\n","Original Sentence:  If you do stop, the script then proceeds to load the entire file into memory.\n"," Stemmed Sentence:  If you do stop , the script then proce to load the entir file into memori . \n","\n","Original Sentence:  Which could be a problem for you.\n"," Stemmed Sentence:  which could be a problem for you . \n","\n","Original Sentence:  You can, however, verify the text by batches of lines, in order to satisfy your curiousity that something good happened as a result of running the first script.\n"," Stemmed Sentence:  you can , howev , verifi the text by batch of line , in order to satisfi your curious that someth good happen as a result of run the first script . \n","\n","Original Sentence:  If you are planning on working on such a large text file, you may need some workarounds for its large size in comparison to your machine's memory.\n"," Stemmed Sentence:  If you are plan on work on such a larg text file , you may need some workaround for it larg size in comparison to your machin 's memori . \n"]}]},{"cell_type":"markdown","metadata":{"id":"aHEm1UNCsx6b"},"source":["**Wordnet Lemmatizer**"]},{"cell_type":"code","metadata":{"id":"qCdf9kEeKQLs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636967110287,"user_tz":-360,"elapsed":2230,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"19497dae-910e-43a3-94fd-23101538dab8"},"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","wordnet_lemmatizer = WordNetLemmatizer()\n","from nltk.tokenize import word_tokenize\n","\n","file = open(\"nlp_preprocessing.txt\")\n","#print(file.read())\n","nlp_preprocessing_text = file.read()\n","#print(nlp_preprocessing_text)\n","#sentence_list\n","'''\n","newline_removed_sentence_list = []\n","for elements in sentence_list:\n","  newline_removed_sentence_list.append(elements.strip())   # strip() - removes the white-spaces and new-line from a list\n","#newline_removed_sentence_list\n","'''\n","punctuations = \".!?,;:\"\n","# To remove punctuations we need to tokenize the sentences into words\n","tokenized_nlp_text = word_tokenize(nlp_preprocessing_text)\n","print(\"Tokenized_text   : \", tokenized_nlp_text)\n","\n","#Remove punctuations----\n","punctuation_removed_text = []\n","for words in tokenized_nlp_text:\n","  if words not in punctuations:\n","    punctuation_removed_text.append(words)\n","print(\"Punc_Removed   : \", punctuation_removed_text)\n","\n","# Stemming the words-------\n","stemmed_words = []\n","for words in punctuation_removed_text:\n","  stemmed_words.append(wordnet_lemmatizer.lemmatize(words))\n","print(\"Stemmed_words: \", stemmed_words)\n","\n"," "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","Tokenized_text   :  ['One', 'of', 'the', 'first', 'things', 'required', 'for', 'natural', 'language', 'processing', '(', 'NLP', ')', 'tasks', 'is', 'a', 'corpus', '.', 'In', 'linguistics', 'and', 'NLP', ',', 'corpus', 'refers', 'to', 'a', 'collection', 'of', 'texts', '.', 'Such', 'collections', 'may', 'be', 'formed', 'of', 'a', 'single', 'language', 'of', 'texts', ',', 'or', 'can', 'span', 'multiple', 'languages', '-', 'there', 'are', 'numerous', 'reasons', 'for', 'which', 'multilingual', 'corpora', ',', 'the', 'plural', 'of', 'corpus', ',', 'may', 'be', 'useful', '.', 'Corpora', 'may', 'also', 'consist', 'of', 'themed', 'texts', ',', 'like-', 'historical', ',', 'Biblical', ',', 'etc', '.', 'Corpora', 'are', 'generally', 'solely', 'used', 'for', 'statistical', 'linguistic', 'analysis', 'and', 'hypothesis', 'testing', '.', 'The', 'good', 'thing', 'is', 'that', 'the', 'internet', 'is', 'filled', 'with', 'text', ',', 'and', 'in', 'many', 'cases', 'this', 'text', 'is', 'collected', 'and', 'well', 'oganized', ',', 'even', 'if', 'it', 'requires', 'some', 'finessing', 'into', 'a', 'more', 'usable', ',', 'precisely-defined', 'format', '.', 'Wikipedia', ',', 'in', 'particular', ',', 'is', 'a', 'rich', 'source', 'of', 'well-organized', 'textual', 'data', '.', 'It', \"'s\", 'also', 'a', 'vast', 'collection', 'of', 'knowledge', ',', 'and', 'the', 'unhampered', 'mind', 'can', 'dream', 'up', 'all', 'sorts', 'of', 'uses', 'for', 'just', 'such', 'a', 'body', 'of', 'text', '.', 'What', 'we', 'will', 'do', 'here', 'is', 'build', 'a', 'corpus', 'from', 'the', 'set', 'of', 'English', 'Wikipedia', 'articles', ',', 'which', 'is', 'freely', 'and', 'conveniently', 'available', 'online', '.', 'In', 'order', 'to', 'easily', 'build', 'a', 'text', 'corpus', 'void', 'of', 'the', 'Wikipedia', 'article', 'markup', ',', 'we', 'will', 'use', 'gensim', ',', 'a', 'topic', 'modeling', 'library', 'for', 'Python', '.', 'A', 'Wikipedia', 'dump', 'file', 'is', 'also', 'required', 'for', 'this', 'procedure', ',', 'quite', 'obviously', '.', 'The', 'latest', 'such', 'files', 'can', 'be', 'found', 'here', '.', 'I', 'wrote', 'a', 'simple', 'Python', 'script', 'to', 'build', 'the', 'corpus', 'by', 'stripping', 'all', 'Wikipedia', 'markup', 'from', 'the', 'articles', ',', 'using', 'gensim', '.', 'You', 'can', 'read', 'up', 'on', 'the', 'WikiCorpus', 'class', 'here', '.', 'A', 'second', 'script', 'then', 'checks', 'the', 'corpus', 'text', 'file', 'we', 'just', 'built', '.', 'Now', ',', 'keep', 'in', 'mind', 'that', 'this', 'large', 'Wikipedia', 'dump', 'file', 'then', 'resulted', 'in', 'a', 'very', 'large', 'corpus', 'file', '.', 'Given', 'its', 'enormous', 'size', ',', 'you', 'may', 'have', 'dificulty', 'reading', 'the', 'full', 'file', 'into', 'memory', 'at', 'one', 'time', '.', 'This', 'script', ',', 'then', ',', 'starts', 'by', 'reading', '50', 'lines', '-', 'which', 'equates', 'to', '50', 'full', 'articles', '-', 'from', 'the', 'text', 'file', 'and', 'outputting', 'them', 'to', 'the', 'terminal', ',', 'after', 'which', 'you', 'can', 'press', 'a', 'key', 'to', 'output', 'another', '50', ',', 'or', 'type', \"'STOP\", \"'\", 'to', 'quit', '.', 'If', 'you', 'do', 'stop', ',', 'the', 'script', 'then', 'proceeds', 'to', 'load', 'the', 'entire', 'file', 'into', 'memory', '.', 'Which', 'could', 'be', 'a', 'problem', 'for', 'you', '.', 'You', 'can', ',', 'however', ',', 'verify', 'the', 'text', 'by', 'batches', 'of', 'lines', ',', 'in', 'order', 'to', 'satisfy', 'your', 'curiousity', 'that', 'something', 'good', 'happened', 'as', 'a', 'result', 'of', 'running', 'the', 'first', 'script', '.', 'If', 'you', 'are', 'planning', 'on', 'working', 'on', 'such', 'a', 'large', 'text', 'file', ',', 'you', 'may', 'need', 'some', 'workarounds', 'for', 'its', 'large', 'size', 'in', 'comparison', 'to', 'your', 'machine', \"'s\", 'memory', '.']\n","Punc_Removed   :  ['One', 'of', 'the', 'first', 'things', 'required', 'for', 'natural', 'language', 'processing', '(', 'NLP', ')', 'tasks', 'is', 'a', 'corpus', 'In', 'linguistics', 'and', 'NLP', 'corpus', 'refers', 'to', 'a', 'collection', 'of', 'texts', 'Such', 'collections', 'may', 'be', 'formed', 'of', 'a', 'single', 'language', 'of', 'texts', 'or', 'can', 'span', 'multiple', 'languages', '-', 'there', 'are', 'numerous', 'reasons', 'for', 'which', 'multilingual', 'corpora', 'the', 'plural', 'of', 'corpus', 'may', 'be', 'useful', 'Corpora', 'may', 'also', 'consist', 'of', 'themed', 'texts', 'like-', 'historical', 'Biblical', 'etc', 'Corpora', 'are', 'generally', 'solely', 'used', 'for', 'statistical', 'linguistic', 'analysis', 'and', 'hypothesis', 'testing', 'The', 'good', 'thing', 'is', 'that', 'the', 'internet', 'is', 'filled', 'with', 'text', 'and', 'in', 'many', 'cases', 'this', 'text', 'is', 'collected', 'and', 'well', 'oganized', 'even', 'if', 'it', 'requires', 'some', 'finessing', 'into', 'a', 'more', 'usable', 'precisely-defined', 'format', 'Wikipedia', 'in', 'particular', 'is', 'a', 'rich', 'source', 'of', 'well-organized', 'textual', 'data', 'It', \"'s\", 'also', 'a', 'vast', 'collection', 'of', 'knowledge', 'and', 'the', 'unhampered', 'mind', 'can', 'dream', 'up', 'all', 'sorts', 'of', 'uses', 'for', 'just', 'such', 'a', 'body', 'of', 'text', 'What', 'we', 'will', 'do', 'here', 'is', 'build', 'a', 'corpus', 'from', 'the', 'set', 'of', 'English', 'Wikipedia', 'articles', 'which', 'is', 'freely', 'and', 'conveniently', 'available', 'online', 'In', 'order', 'to', 'easily', 'build', 'a', 'text', 'corpus', 'void', 'of', 'the', 'Wikipedia', 'article', 'markup', 'we', 'will', 'use', 'gensim', 'a', 'topic', 'modeling', 'library', 'for', 'Python', 'A', 'Wikipedia', 'dump', 'file', 'is', 'also', 'required', 'for', 'this', 'procedure', 'quite', 'obviously', 'The', 'latest', 'such', 'files', 'can', 'be', 'found', 'here', 'I', 'wrote', 'a', 'simple', 'Python', 'script', 'to', 'build', 'the', 'corpus', 'by', 'stripping', 'all', 'Wikipedia', 'markup', 'from', 'the', 'articles', 'using', 'gensim', 'You', 'can', 'read', 'up', 'on', 'the', 'WikiCorpus', 'class', 'here', 'A', 'second', 'script', 'then', 'checks', 'the', 'corpus', 'text', 'file', 'we', 'just', 'built', 'Now', 'keep', 'in', 'mind', 'that', 'this', 'large', 'Wikipedia', 'dump', 'file', 'then', 'resulted', 'in', 'a', 'very', 'large', 'corpus', 'file', 'Given', 'its', 'enormous', 'size', 'you', 'may', 'have', 'dificulty', 'reading', 'the', 'full', 'file', 'into', 'memory', 'at', 'one', 'time', 'This', 'script', 'then', 'starts', 'by', 'reading', '50', 'lines', '-', 'which', 'equates', 'to', '50', 'full', 'articles', '-', 'from', 'the', 'text', 'file', 'and', 'outputting', 'them', 'to', 'the', 'terminal', 'after', 'which', 'you', 'can', 'press', 'a', 'key', 'to', 'output', 'another', '50', 'or', 'type', \"'STOP\", \"'\", 'to', 'quit', 'If', 'you', 'do', 'stop', 'the', 'script', 'then', 'proceeds', 'to', 'load', 'the', 'entire', 'file', 'into', 'memory', 'Which', 'could', 'be', 'a', 'problem', 'for', 'you', 'You', 'can', 'however', 'verify', 'the', 'text', 'by', 'batches', 'of', 'lines', 'in', 'order', 'to', 'satisfy', 'your', 'curiousity', 'that', 'something', 'good', 'happened', 'as', 'a', 'result', 'of', 'running', 'the', 'first', 'script', 'If', 'you', 'are', 'planning', 'on', 'working', 'on', 'such', 'a', 'large', 'text', 'file', 'you', 'may', 'need', 'some', 'workarounds', 'for', 'its', 'large', 'size', 'in', 'comparison', 'to', 'your', 'machine', \"'s\", 'memory']\n","Stemmed_words:  ['One', 'of', 'the', 'first', 'thing', 'required', 'for', 'natural', 'language', 'processing', '(', 'NLP', ')', 'task', 'is', 'a', 'corpus', 'In', 'linguistics', 'and', 'NLP', 'corpus', 'refers', 'to', 'a', 'collection', 'of', 'text', 'Such', 'collection', 'may', 'be', 'formed', 'of', 'a', 'single', 'language', 'of', 'text', 'or', 'can', 'span', 'multiple', 'language', '-', 'there', 'are', 'numerous', 'reason', 'for', 'which', 'multilingual', 'corpus', 'the', 'plural', 'of', 'corpus', 'may', 'be', 'useful', 'Corpora', 'may', 'also', 'consist', 'of', 'themed', 'text', 'like-', 'historical', 'Biblical', 'etc', 'Corpora', 'are', 'generally', 'solely', 'used', 'for', 'statistical', 'linguistic', 'analysis', 'and', 'hypothesis', 'testing', 'The', 'good', 'thing', 'is', 'that', 'the', 'internet', 'is', 'filled', 'with', 'text', 'and', 'in', 'many', 'case', 'this', 'text', 'is', 'collected', 'and', 'well', 'oganized', 'even', 'if', 'it', 'requires', 'some', 'finessing', 'into', 'a', 'more', 'usable', 'precisely-defined', 'format', 'Wikipedia', 'in', 'particular', 'is', 'a', 'rich', 'source', 'of', 'well-organized', 'textual', 'data', 'It', \"'s\", 'also', 'a', 'vast', 'collection', 'of', 'knowledge', 'and', 'the', 'unhampered', 'mind', 'can', 'dream', 'up', 'all', 'sort', 'of', 'us', 'for', 'just', 'such', 'a', 'body', 'of', 'text', 'What', 'we', 'will', 'do', 'here', 'is', 'build', 'a', 'corpus', 'from', 'the', 'set', 'of', 'English', 'Wikipedia', 'article', 'which', 'is', 'freely', 'and', 'conveniently', 'available', 'online', 'In', 'order', 'to', 'easily', 'build', 'a', 'text', 'corpus', 'void', 'of', 'the', 'Wikipedia', 'article', 'markup', 'we', 'will', 'use', 'gensim', 'a', 'topic', 'modeling', 'library', 'for', 'Python', 'A', 'Wikipedia', 'dump', 'file', 'is', 'also', 'required', 'for', 'this', 'procedure', 'quite', 'obviously', 'The', 'latest', 'such', 'file', 'can', 'be', 'found', 'here', 'I', 'wrote', 'a', 'simple', 'Python', 'script', 'to', 'build', 'the', 'corpus', 'by', 'stripping', 'all', 'Wikipedia', 'markup', 'from', 'the', 'article', 'using', 'gensim', 'You', 'can', 'read', 'up', 'on', 'the', 'WikiCorpus', 'class', 'here', 'A', 'second', 'script', 'then', 'check', 'the', 'corpus', 'text', 'file', 'we', 'just', 'built', 'Now', 'keep', 'in', 'mind', 'that', 'this', 'large', 'Wikipedia', 'dump', 'file', 'then', 'resulted', 'in', 'a', 'very', 'large', 'corpus', 'file', 'Given', 'it', 'enormous', 'size', 'you', 'may', 'have', 'dificulty', 'reading', 'the', 'full', 'file', 'into', 'memory', 'at', 'one', 'time', 'This', 'script', 'then', 'start', 'by', 'reading', '50', 'line', '-', 'which', 'equates', 'to', '50', 'full', 'article', '-', 'from', 'the', 'text', 'file', 'and', 'outputting', 'them', 'to', 'the', 'terminal', 'after', 'which', 'you', 'can', 'press', 'a', 'key', 'to', 'output', 'another', '50', 'or', 'type', \"'STOP\", \"'\", 'to', 'quit', 'If', 'you', 'do', 'stop', 'the', 'script', 'then', 'proceeds', 'to', 'load', 'the', 'entire', 'file', 'into', 'memory', 'Which', 'could', 'be', 'a', 'problem', 'for', 'you', 'You', 'can', 'however', 'verify', 'the', 'text', 'by', 'batch', 'of', 'line', 'in', 'order', 'to', 'satisfy', 'your', 'curiousity', 'that', 'something', 'good', 'happened', 'a', 'a', 'result', 'of', 'running', 'the', 'first', 'script', 'If', 'you', 'are', 'planning', 'on', 'working', 'on', 'such', 'a', 'large', 'text', 'file', 'you', 'may', 'need', 'some', 'workarounds', 'for', 'it', 'large', 'size', 'in', 'comparison', 'to', 'your', 'machine', \"'s\", 'memory']\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j8K0SpUZiXu2","executionInfo":{"status":"ok","timestamp":1636967117870,"user_tz":-360,"elapsed":669,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"95e04f78-ff59-4940-d148-dc951f25fcfb"},"source":["print(\"{0:20}:{1:20}\".format(\"Word\",\"Lemma\"))\n","for word in punctuation_removed_text:\n","    print (\"{} : {}\".format(word,wordnet_lemmatizer.lemmatize(word)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Word                :Lemma               \n","One : One\n","of : of\n","the : the\n","first : first\n","things : thing\n","required : required\n","for : for\n","natural : natural\n","language : language\n","processing : processing\n","( : (\n","NLP : NLP\n",") : )\n","tasks : task\n","is : is\n","a : a\n","corpus : corpus\n","In : In\n","linguistics : linguistics\n","and : and\n","NLP : NLP\n","corpus : corpus\n","refers : refers\n","to : to\n","a : a\n","collection : collection\n","of : of\n","texts : text\n","Such : Such\n","collections : collection\n","may : may\n","be : be\n","formed : formed\n","of : of\n","a : a\n","single : single\n","language : language\n","of : of\n","texts : text\n","or : or\n","can : can\n","span : span\n","multiple : multiple\n","languages : language\n","- : -\n","there : there\n","are : are\n","numerous : numerous\n","reasons : reason\n","for : for\n","which : which\n","multilingual : multilingual\n","corpora : corpus\n","the : the\n","plural : plural\n","of : of\n","corpus : corpus\n","may : may\n","be : be\n","useful : useful\n","Corpora : Corpora\n","may : may\n","also : also\n","consist : consist\n","of : of\n","themed : themed\n","texts : text\n","like- : like-\n","historical : historical\n","Biblical : Biblical\n","etc : etc\n","Corpora : Corpora\n","are : are\n","generally : generally\n","solely : solely\n","used : used\n","for : for\n","statistical : statistical\n","linguistic : linguistic\n","analysis : analysis\n","and : and\n","hypothesis : hypothesis\n","testing : testing\n","The : The\n","good : good\n","thing : thing\n","is : is\n","that : that\n","the : the\n","internet : internet\n","is : is\n","filled : filled\n","with : with\n","text : text\n","and : and\n","in : in\n","many : many\n","cases : case\n","this : this\n","text : text\n","is : is\n","collected : collected\n","and : and\n","well : well\n","oganized : oganized\n","even : even\n","if : if\n","it : it\n","requires : requires\n","some : some\n","finessing : finessing\n","into : into\n","a : a\n","more : more\n","usable : usable\n","precisely-defined : precisely-defined\n","format : format\n","Wikipedia : Wikipedia\n","in : in\n","particular : particular\n","is : is\n","a : a\n","rich : rich\n","source : source\n","of : of\n","well-organized : well-organized\n","textual : textual\n","data : data\n","It : It\n","'s : 's\n","also : also\n","a : a\n","vast : vast\n","collection : collection\n","of : of\n","knowledge : knowledge\n","and : and\n","the : the\n","unhampered : unhampered\n","mind : mind\n","can : can\n","dream : dream\n","up : up\n","all : all\n","sorts : sort\n","of : of\n","uses : us\n","for : for\n","just : just\n","such : such\n","a : a\n","body : body\n","of : of\n","text : text\n","What : What\n","we : we\n","will : will\n","do : do\n","here : here\n","is : is\n","build : build\n","a : a\n","corpus : corpus\n","from : from\n","the : the\n","set : set\n","of : of\n","English : English\n","Wikipedia : Wikipedia\n","articles : article\n","which : which\n","is : is\n","freely : freely\n","and : and\n","conveniently : conveniently\n","available : available\n","online : online\n","In : In\n","order : order\n","to : to\n","easily : easily\n","build : build\n","a : a\n","text : text\n","corpus : corpus\n","void : void\n","of : of\n","the : the\n","Wikipedia : Wikipedia\n","article : article\n","markup : markup\n","we : we\n","will : will\n","use : use\n","gensim : gensim\n","a : a\n","topic : topic\n","modeling : modeling\n","library : library\n","for : for\n","Python : Python\n","A : A\n","Wikipedia : Wikipedia\n","dump : dump\n","file : file\n","is : is\n","also : also\n","required : required\n","for : for\n","this : this\n","procedure : procedure\n","quite : quite\n","obviously : obviously\n","The : The\n","latest : latest\n","such : such\n","files : file\n","can : can\n","be : be\n","found : found\n","here : here\n","I : I\n","wrote : wrote\n","a : a\n","simple : simple\n","Python : Python\n","script : script\n","to : to\n","build : build\n","the : the\n","corpus : corpus\n","by : by\n","stripping : stripping\n","all : all\n","Wikipedia : Wikipedia\n","markup : markup\n","from : from\n","the : the\n","articles : article\n","using : using\n","gensim : gensim\n","You : You\n","can : can\n","read : read\n","up : up\n","on : on\n","the : the\n","WikiCorpus : WikiCorpus\n","class : class\n","here : here\n","A : A\n","second : second\n","script : script\n","then : then\n","checks : check\n","the : the\n","corpus : corpus\n","text : text\n","file : file\n","we : we\n","just : just\n","built : built\n","Now : Now\n","keep : keep\n","in : in\n","mind : mind\n","that : that\n","this : this\n","large : large\n","Wikipedia : Wikipedia\n","dump : dump\n","file : file\n","then : then\n","resulted : resulted\n","in : in\n","a : a\n","very : very\n","large : large\n","corpus : corpus\n","file : file\n","Given : Given\n","its : it\n","enormous : enormous\n","size : size\n","you : you\n","may : may\n","have : have\n","dificulty : dificulty\n","reading : reading\n","the : the\n","full : full\n","file : file\n","into : into\n","memory : memory\n","at : at\n","one : one\n","time : time\n","This : This\n","script : script\n","then : then\n","starts : start\n","by : by\n","reading : reading\n","50 : 50\n","lines : line\n","- : -\n","which : which\n","equates : equates\n","to : to\n","50 : 50\n","full : full\n","articles : article\n","- : -\n","from : from\n","the : the\n","text : text\n","file : file\n","and : and\n","outputting : outputting\n","them : them\n","to : to\n","the : the\n","terminal : terminal\n","after : after\n","which : which\n","you : you\n","can : can\n","press : press\n","a : a\n","key : key\n","to : to\n","output : output\n","another : another\n","50 : 50\n","or : or\n","type : type\n","'STOP : 'STOP\n","' : '\n","to : to\n","quit : quit\n","If : If\n","you : you\n","do : do\n","stop : stop\n","the : the\n","script : script\n","then : then\n","proceeds : proceeds\n","to : to\n","load : load\n","the : the\n","entire : entire\n","file : file\n","into : into\n","memory : memory\n","Which : Which\n","could : could\n","be : be\n","a : a\n","problem : problem\n","for : for\n","you : you\n","You : You\n","can : can\n","however : however\n","verify : verify\n","the : the\n","text : text\n","by : by\n","batches : batch\n","of : of\n","lines : line\n","in : in\n","order : order\n","to : to\n","satisfy : satisfy\n","your : your\n","curiousity : curiousity\n","that : that\n","something : something\n","good : good\n","happened : happened\n","as : a\n","a : a\n","result : result\n","of : of\n","running : running\n","the : the\n","first : first\n","script : script\n","If : If\n","you : you\n","are : are\n","planning : planning\n","on : on\n","working : working\n","on : on\n","such : such\n","a : a\n","large : large\n","text : text\n","file : file\n","you : you\n","may : may\n","need : need\n","some : some\n","workarounds : workarounds\n","for : for\n","its : it\n","large : large\n","size : size\n","in : in\n","comparison : comparison\n","to : to\n","your : your\n","machine : machine\n","'s : 's\n","memory : memory\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bcWpApJjpM-P","executionInfo":{"status":"ok","timestamp":1636967125145,"user_tz":-360,"elapsed":439,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"e593bfa8-2dd1-4ba7-967e-fba24c88138a"},"source":["# Lemmatize specific parts-of-speech from thye text\n","for word in punctuation_removed_text:\n","    print(\"{} : {}\".format(word,wordnet_lemmatizer.lemmatize(word, pos=\"v\")))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["One : One\n","of : of\n","the : the\n","first : first\n","things : things\n","required : require\n","for : for\n","natural : natural\n","language : language\n","processing : process\n","( : (\n","NLP : NLP\n",") : )\n","tasks : task\n","is : be\n","a : a\n","corpus : corpus\n","In : In\n","linguistics : linguistics\n","and : and\n","NLP : NLP\n","corpus : corpus\n","refers : refer\n","to : to\n","a : a\n","collection : collection\n","of : of\n","texts : texts\n","Such : Such\n","collections : collections\n","may : may\n","be : be\n","formed : form\n","of : of\n","a : a\n","single : single\n","language : language\n","of : of\n","texts : texts\n","or : or\n","can : can\n","span : span\n","multiple : multiple\n","languages : languages\n","- : -\n","there : there\n","are : be\n","numerous : numerous\n","reasons : reason\n","for : for\n","which : which\n","multilingual : multilingual\n","corpora : corpora\n","the : the\n","plural : plural\n","of : of\n","corpus : corpus\n","may : may\n","be : be\n","useful : useful\n","Corpora : Corpora\n","may : may\n","also : also\n","consist : consist\n","of : of\n","themed : theme\n","texts : texts\n","like- : like-\n","historical : historical\n","Biblical : Biblical\n","etc : etc\n","Corpora : Corpora\n","are : be\n","generally : generally\n","solely : solely\n","used : use\n","for : for\n","statistical : statistical\n","linguistic : linguistic\n","analysis : analysis\n","and : and\n","hypothesis : hypothesis\n","testing : test\n","The : The\n","good : good\n","thing : thing\n","is : be\n","that : that\n","the : the\n","internet : internet\n","is : be\n","filled : fill\n","with : with\n","text : text\n","and : and\n","in : in\n","many : many\n","cases : case\n","this : this\n","text : text\n","is : be\n","collected : collect\n","and : and\n","well : well\n","oganized : oganized\n","even : even\n","if : if\n","it : it\n","requires : require\n","some : some\n","finessing : fin\n","into : into\n","a : a\n","more : more\n","usable : usable\n","precisely-defined : precisely-defined\n","format : format\n","Wikipedia : Wikipedia\n","in : in\n","particular : particular\n","is : be\n","a : a\n","rich : rich\n","source : source\n","of : of\n","well-organized : well-organized\n","textual : textual\n","data : data\n","It : It\n","'s : 's\n","also : also\n","a : a\n","vast : vast\n","collection : collection\n","of : of\n","knowledge : knowledge\n","and : and\n","the : the\n","unhampered : unhampered\n","mind : mind\n","can : can\n","dream : dream\n","up : up\n","all : all\n","sorts : sort\n","of : of\n","uses : use\n","for : for\n","just : just\n","such : such\n","a : a\n","body : body\n","of : of\n","text : text\n","What : What\n","we : we\n","will : will\n","do : do\n","here : here\n","is : be\n","build : build\n","a : a\n","corpus : corpus\n","from : from\n","the : the\n","set : set\n","of : of\n","English : English\n","Wikipedia : Wikipedia\n","articles : article\n","which : which\n","is : be\n","freely : freely\n","and : and\n","conveniently : conveniently\n","available : available\n","online : online\n","In : In\n","order : order\n","to : to\n","easily : easily\n","build : build\n","a : a\n","text : text\n","corpus : corpus\n","void : void\n","of : of\n","the : the\n","Wikipedia : Wikipedia\n","article : article\n","markup : markup\n","we : we\n","will : will\n","use : use\n","gensim : gensim\n","a : a\n","topic : topic\n","modeling : model\n","library : library\n","for : for\n","Python : Python\n","A : A\n","Wikipedia : Wikipedia\n","dump : dump\n","file : file\n","is : be\n","also : also\n","required : require\n","for : for\n","this : this\n","procedure : procedure\n","quite : quite\n","obviously : obviously\n","The : The\n","latest : latest\n","such : such\n","files : file\n","can : can\n","be : be\n","found : find\n","here : here\n","I : I\n","wrote : write\n","a : a\n","simple : simple\n","Python : Python\n","script : script\n","to : to\n","build : build\n","the : the\n","corpus : corpus\n","by : by\n","stripping : strip\n","all : all\n","Wikipedia : Wikipedia\n","markup : markup\n","from : from\n","the : the\n","articles : article\n","using : use\n","gensim : gensim\n","You : You\n","can : can\n","read : read\n","up : up\n","on : on\n","the : the\n","WikiCorpus : WikiCorpus\n","class : class\n","here : here\n","A : A\n","second : second\n","script : script\n","then : then\n","checks : check\n","the : the\n","corpus : corpus\n","text : text\n","file : file\n","we : we\n","just : just\n","built : build\n","Now : Now\n","keep : keep\n","in : in\n","mind : mind\n","that : that\n","this : this\n","large : large\n","Wikipedia : Wikipedia\n","dump : dump\n","file : file\n","then : then\n","resulted : result\n","in : in\n","a : a\n","very : very\n","large : large\n","corpus : corpus\n","file : file\n","Given : Given\n","its : its\n","enormous : enormous\n","size : size\n","you : you\n","may : may\n","have : have\n","dificulty : dificulty\n","reading : read\n","the : the\n","full : full\n","file : file\n","into : into\n","memory : memory\n","at : at\n","one : one\n","time : time\n","This : This\n","script : script\n","then : then\n","starts : start\n","by : by\n","reading : read\n","50 : 50\n","lines : line\n","- : -\n","which : which\n","equates : equate\n","to : to\n","50 : 50\n","full : full\n","articles : article\n","- : -\n","from : from\n","the : the\n","text : text\n","file : file\n","and : and\n","outputting : output\n","them : them\n","to : to\n","the : the\n","terminal : terminal\n","after : after\n","which : which\n","you : you\n","can : can\n","press : press\n","a : a\n","key : key\n","to : to\n","output : output\n","another : another\n","50 : 50\n","or : or\n","type : type\n","'STOP : 'STOP\n","' : '\n","to : to\n","quit : quit\n","If : If\n","you : you\n","do : do\n","stop : stop\n","the : the\n","script : script\n","then : then\n","proceeds : proceed\n","to : to\n","load : load\n","the : the\n","entire : entire\n","file : file\n","into : into\n","memory : memory\n","Which : Which\n","could : could\n","be : be\n","a : a\n","problem : problem\n","for : for\n","you : you\n","You : You\n","can : can\n","however : however\n","verify : verify\n","the : the\n","text : text\n","by : by\n","batches : batch\n","of : of\n","lines : line\n","in : in\n","order : order\n","to : to\n","satisfy : satisfy\n","your : your\n","curiousity : curiousity\n","that : that\n","something : something\n","good : good\n","happened : happen\n","as : as\n","a : a\n","result : result\n","of : of\n","running : run\n","the : the\n","first : first\n","script : script\n","If : If\n","you : you\n","are : be\n","planning : plan\n","on : on\n","working : work\n","on : on\n","such : such\n","a : a\n","large : large\n","text : text\n","file : file\n","you : you\n","may : may\n","need : need\n","some : some\n","workarounds : workarounds\n","for : for\n","its : its\n","large : large\n","size : size\n","in : in\n","comparison : comparison\n","to : to\n","your : your\n","machine : machine\n","'s : 's\n","memory : memory\n"]}]},{"cell_type":"markdown","metadata":{"id":"IpcXl8LkMV_6"},"source":["# **POS-Tag** \n","    Parts-of-Speech(PoS) tagging is a part of NLP, refers to - catagorizing words to the corresponding particular part of speech, depending on the definition of the word and its context in the sentence."]},{"cell_type":"markdown","metadata":{"id":"OOGVEpYYQ1ok"},"source":["*For PoS Tagging we'll use nltk. To do that - \n","\n","Open your terminal and run --\n","\n","> pip install nltk\n","\n","Before using NLTK, To import all the packages--\n","\n","> impoxt nltk\n","\n","> nltk.download()\n","\n","It will appear a windows. Choose 'all' for all the packages and  click 'download'.*\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6CN2hbCOVLkY"},"source":["In corpus linguistics, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging or word-category disambiguation.\n","\n","> Input: Everything is all about money.\n","\n","> Output: [('Everything', 'NN'), ('is', 'VBZ'), ('all', 'DT'),('about', 'IN')('money', 'NN'), ('.', '.')]\n","\n","Here’s a list of the tags, what they mean, and some examples:\n","\n","> CC - Coordinating Conjunction\n","\n","> CD - cardinal digit\n","\n","> DT - determiner\n","\n","> EX - existential there (like: “there is” … think of it like “there exists”)\n","\n","> FW - foreign word\n","\n","> IN - preposition/subordinating conjunction\n","\n","> JJ - adjective ‘big’\n","\n","> JJR - adjective, comparative ‘bigger’\n","\n","> JJS - adjective, superlative ‘biggest’\n","\n","> LS - list marker 1)\n","\n","> MD - modal could, will\n","\n","> NN - noun, singular ‘desk’\n","\n","> NNS - noun plural ‘desks’\n","\n","> NNP - proper noun, singular ‘Harrison’\n","\n","> NNPS - proper noun, plural ‘Americans’\n","\n","> PDT - predeterminer ‘all the kids’\n","\n","> POS - possessive ending parent‘s\n","\n","> PRP - personal pronoun I, he, she\n","\n","> PRP$ - possessive pronoun my, his, hers\n","\n","> RB - adverb very, silently,\n","\n","> RBR - adverb, comparative better\n","\n","> RBS - adverb, superlative best\n","\n","> RP - particle give up\n","\n","> TO - to go ‘to‘ the store.\n","\n","> UH - interjection errrrrrrrm\n","\n","> VB - verb, base form take\n","\n","> VBD - verb, past tense took\n","\n","> VBG - verb, gerund/present participle taking\n","\n","> VBN - verb, past participle taken\n","\n","> VBP - verb, sing. present, non-3d take\n","\n","> VBZ - verb, 3rd person sing. present takes\n","\n","> WDT - wh-determiner which\n","\n","> WP - wh-pronoun who, what\n","\n","> WP$ - possessive wh-pronoun whose\n","\n","> WRB - wh-abverb where, when"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZQ63GGOQa_C6","executionInfo":{"status":"ok","timestamp":1637057213007,"user_tz":-360,"elapsed":759,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"891a1c94-ea55-41ed-eb66-c734992f1c0f"},"source":["# Import and Download nltk----------\n","import nltk\n","import re\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"JV09N6KbMhKy"},"source":["# Import the libraries---------\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","stop_words = set(stopwords.words('english'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fSXkry9qhWb3"},"source":["file = open(\"nlp_preprocessing.txt\")\n","#print(file.read())\n","nlp_preprocessing_text = file.read()\n","#nlp_preprocessing_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u6JLi4uFh2mJ"},"source":["#Splited text------\n","sentences = re.compile('[.!?]').split(nlp_preprocessing_text)\n","#sentences\n","# Remove white space and new line.\n","removed_list = []\n","for elements in sentences:\n","  removed_list.append(elements.strip())   # strip() - removes the white-spaces and new-line from a list\n","#converted_list\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Odle3XtqJ6J6","executionInfo":{"status":"ok","timestamp":1637057238417,"user_tz":-360,"elapsed":586,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"32ef5a22-310b-4688-ec38-dc80e61b0fc0"},"source":["tokenized_text = word_tokenize(nlp_preprocessing_text)\n","print(tokenized_text)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['One', 'of', 'the', 'first', 'things', 'required', 'for', 'natural', 'language', 'processing', '(', 'NLP', ')', 'tasks', 'is', 'a', 'corpus', '.', 'In', 'linguistics', 'and', 'NLP', ',', 'corpus', 'refers', 'to', 'a', 'collection', 'of', 'texts', '.', 'Such', 'collections', 'may', 'be', 'formed', 'of', 'a', 'single', 'language', 'of', 'texts', ',', 'or', 'can', 'span', 'multiple', 'languages', '-', 'there', 'are', 'numerous', 'reasons', 'for', 'which', 'multilingual', 'corpora', ',', 'the', 'plural', 'of', 'corpus', ',', 'may', 'be', 'useful', '.', 'Corpora', 'may', 'also', 'consist', 'of', 'themed', 'texts', ',', 'like-', 'historical', ',', 'Biblical', ',', 'etc', '.', 'Corpora', 'are', 'generally', 'solely', 'used', 'for', 'statistical', 'linguistic', 'analysis', 'and', 'hypothesis', 'testing', '.', 'The', 'good', 'thing', 'is', 'that', 'the', 'internet', 'is', 'filled', 'with', 'text', ',', 'and', 'in', 'many', 'cases', 'this', 'text', 'is', 'collected', 'and', 'well', 'oganized', ',', 'even', 'if', 'it', 'requires', 'some', 'finessing', 'into', 'a', 'more', 'usable', ',', 'precisely-defined', 'format', '.', 'Wikipedia', ',', 'in', 'particular', ',', 'is', 'a', 'rich', 'source', 'of', 'well-organized', 'textual', 'data', '.', 'It', \"'s\", 'also', 'a', 'vast', 'collection', 'of', 'knowledge', ',', 'and', 'the', 'unhampered', 'mind', 'can', 'dream', 'up', 'all', 'sorts', 'of', 'uses', 'for', 'just', 'such', 'a', 'body', 'of', 'text', '.', 'What', 'we', 'will', 'do', 'here', 'is', 'build', 'a', 'corpus', 'from', 'the', 'set', 'of', 'English', 'Wikipedia', 'articles', ',', 'which', 'is', 'freely', 'and', 'conveniently', 'available', 'online', '.', 'In', 'order', 'to', 'easily', 'build', 'a', 'text', 'corpus', 'void', 'of', 'the', 'Wikipedia', 'article', 'markup', ',', 'we', 'will', 'use', 'gensim', ',', 'a', 'topic', 'modeling', 'library', 'for', 'Python', '.', 'A', 'Wikipedia', 'dump', 'file', 'is', 'also', 'required', 'for', 'this', 'procedure', ',', 'quite', 'obviously', '.', 'The', 'latest', 'such', 'files', 'can', 'be', 'found', 'here', '.', 'I', 'wrote', 'a', 'simple', 'Python', 'script', 'to', 'build', 'the', 'corpus', 'by', 'stripping', 'all', 'Wikipedia', 'markup', 'from', 'the', 'articles', ',', 'using', 'gensim', '.', 'You', 'can', 'read', 'up', 'on', 'the', 'WikiCorpus', 'class', 'here', '.', 'A', 'second', 'script', 'then', 'checks', 'the', 'corpus', 'text', 'file', 'we', 'just', 'built', '.', 'Now', ',', 'keep', 'in', 'mind', 'that', 'this', 'large', 'Wikipedia', 'dump', 'file', 'then', 'resulted', 'in', 'a', 'very', 'large', 'corpus', 'file', '.', 'Given', 'its', 'enormous', 'size', ',', 'you', 'may', 'have', 'dificulty', 'reading', 'the', 'full', 'file', 'into', 'memory', 'at', 'one', 'time', '.', 'This', 'script', ',', 'then', ',', 'starts', 'by', 'reading', '50', 'lines', '-', 'which', 'equates', 'to', '50', 'full', 'articles', '-', 'from', 'the', 'text', 'file', 'and', 'outputting', 'them', 'to', 'the', 'terminal', ',', 'after', 'which', 'you', 'can', 'press', 'a', 'key', 'to', 'output', 'another', '50', ',', 'or', 'type', \"'STOP\", \"'\", 'to', 'quit', '.', 'If', 'you', 'do', 'stop', ',', 'the', 'script', 'then', 'proceeds', 'to', 'load', 'the', 'entire', 'file', 'into', 'memory', '.', 'Which', 'could', 'be', 'a', 'problem', 'for', 'you', '.', 'You', 'can', ',', 'however', ',', 'verify', 'the', 'text', 'by', 'batches', 'of', 'lines', ',', 'in', 'order', 'to', 'satisfy', 'your', 'curiousity', 'that', 'something', 'good', 'happened', 'as', 'a', 'result', 'of', 'running', 'the', 'first', 'script', '.', 'If', 'you', 'are', 'planning', 'on', 'working', 'on', 'such', 'a', 'large', 'text', 'file', ',', 'you', 'may', 'need', 'some', 'workarounds', 'for', 'its', 'large', 'size', 'in', 'comparison', 'to', 'your', 'machine', \"'s\", 'memory', '.']\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Z9K9DqrlGHU","executionInfo":{"status":"ok","timestamp":1637057243101,"user_tz":-360,"elapsed":598,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"abbfe6b7-4fe7-470c-a93d-7e800d10f500"},"source":["# Remove the stopwords----------\n","stopped_list = []\n","for word in tokenized_text:\n","  if word not in stop_words:\n","    stopped_list.append(word)\n","    #stopped_list= \"\".join(word)\n","print(tokenized_text)\n","print(stopped_list)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['One', 'of', 'the', 'first', 'things', 'required', 'for', 'natural', 'language', 'processing', '(', 'NLP', ')', 'tasks', 'is', 'a', 'corpus', '.', 'In', 'linguistics', 'and', 'NLP', ',', 'corpus', 'refers', 'to', 'a', 'collection', 'of', 'texts', '.', 'Such', 'collections', 'may', 'be', 'formed', 'of', 'a', 'single', 'language', 'of', 'texts', ',', 'or', 'can', 'span', 'multiple', 'languages', '-', 'there', 'are', 'numerous', 'reasons', 'for', 'which', 'multilingual', 'corpora', ',', 'the', 'plural', 'of', 'corpus', ',', 'may', 'be', 'useful', '.', 'Corpora', 'may', 'also', 'consist', 'of', 'themed', 'texts', ',', 'like-', 'historical', ',', 'Biblical', ',', 'etc', '.', 'Corpora', 'are', 'generally', 'solely', 'used', 'for', 'statistical', 'linguistic', 'analysis', 'and', 'hypothesis', 'testing', '.', 'The', 'good', 'thing', 'is', 'that', 'the', 'internet', 'is', 'filled', 'with', 'text', ',', 'and', 'in', 'many', 'cases', 'this', 'text', 'is', 'collected', 'and', 'well', 'oganized', ',', 'even', 'if', 'it', 'requires', 'some', 'finessing', 'into', 'a', 'more', 'usable', ',', 'precisely-defined', 'format', '.', 'Wikipedia', ',', 'in', 'particular', ',', 'is', 'a', 'rich', 'source', 'of', 'well-organized', 'textual', 'data', '.', 'It', \"'s\", 'also', 'a', 'vast', 'collection', 'of', 'knowledge', ',', 'and', 'the', 'unhampered', 'mind', 'can', 'dream', 'up', 'all', 'sorts', 'of', 'uses', 'for', 'just', 'such', 'a', 'body', 'of', 'text', '.', 'What', 'we', 'will', 'do', 'here', 'is', 'build', 'a', 'corpus', 'from', 'the', 'set', 'of', 'English', 'Wikipedia', 'articles', ',', 'which', 'is', 'freely', 'and', 'conveniently', 'available', 'online', '.', 'In', 'order', 'to', 'easily', 'build', 'a', 'text', 'corpus', 'void', 'of', 'the', 'Wikipedia', 'article', 'markup', ',', 'we', 'will', 'use', 'gensim', ',', 'a', 'topic', 'modeling', 'library', 'for', 'Python', '.', 'A', 'Wikipedia', 'dump', 'file', 'is', 'also', 'required', 'for', 'this', 'procedure', ',', 'quite', 'obviously', '.', 'The', 'latest', 'such', 'files', 'can', 'be', 'found', 'here', '.', 'I', 'wrote', 'a', 'simple', 'Python', 'script', 'to', 'build', 'the', 'corpus', 'by', 'stripping', 'all', 'Wikipedia', 'markup', 'from', 'the', 'articles', ',', 'using', 'gensim', '.', 'You', 'can', 'read', 'up', 'on', 'the', 'WikiCorpus', 'class', 'here', '.', 'A', 'second', 'script', 'then', 'checks', 'the', 'corpus', 'text', 'file', 'we', 'just', 'built', '.', 'Now', ',', 'keep', 'in', 'mind', 'that', 'this', 'large', 'Wikipedia', 'dump', 'file', 'then', 'resulted', 'in', 'a', 'very', 'large', 'corpus', 'file', '.', 'Given', 'its', 'enormous', 'size', ',', 'you', 'may', 'have', 'dificulty', 'reading', 'the', 'full', 'file', 'into', 'memory', 'at', 'one', 'time', '.', 'This', 'script', ',', 'then', ',', 'starts', 'by', 'reading', '50', 'lines', '-', 'which', 'equates', 'to', '50', 'full', 'articles', '-', 'from', 'the', 'text', 'file', 'and', 'outputting', 'them', 'to', 'the', 'terminal', ',', 'after', 'which', 'you', 'can', 'press', 'a', 'key', 'to', 'output', 'another', '50', ',', 'or', 'type', \"'STOP\", \"'\", 'to', 'quit', '.', 'If', 'you', 'do', 'stop', ',', 'the', 'script', 'then', 'proceeds', 'to', 'load', 'the', 'entire', 'file', 'into', 'memory', '.', 'Which', 'could', 'be', 'a', 'problem', 'for', 'you', '.', 'You', 'can', ',', 'however', ',', 'verify', 'the', 'text', 'by', 'batches', 'of', 'lines', ',', 'in', 'order', 'to', 'satisfy', 'your', 'curiousity', 'that', 'something', 'good', 'happened', 'as', 'a', 'result', 'of', 'running', 'the', 'first', 'script', '.', 'If', 'you', 'are', 'planning', 'on', 'working', 'on', 'such', 'a', 'large', 'text', 'file', ',', 'you', 'may', 'need', 'some', 'workarounds', 'for', 'its', 'large', 'size', 'in', 'comparison', 'to', 'your', 'machine', \"'s\", 'memory', '.']\n","['One', 'first', 'things', 'required', 'natural', 'language', 'processing', '(', 'NLP', ')', 'tasks', 'corpus', '.', 'In', 'linguistics', 'NLP', ',', 'corpus', 'refers', 'collection', 'texts', '.', 'Such', 'collections', 'may', 'formed', 'single', 'language', 'texts', ',', 'span', 'multiple', 'languages', '-', 'numerous', 'reasons', 'multilingual', 'corpora', ',', 'plural', 'corpus', ',', 'may', 'useful', '.', 'Corpora', 'may', 'also', 'consist', 'themed', 'texts', ',', 'like-', 'historical', ',', 'Biblical', ',', 'etc', '.', 'Corpora', 'generally', 'solely', 'used', 'statistical', 'linguistic', 'analysis', 'hypothesis', 'testing', '.', 'The', 'good', 'thing', 'internet', 'filled', 'text', ',', 'many', 'cases', 'text', 'collected', 'well', 'oganized', ',', 'even', 'requires', 'finessing', 'usable', ',', 'precisely-defined', 'format', '.', 'Wikipedia', ',', 'particular', ',', 'rich', 'source', 'well-organized', 'textual', 'data', '.', 'It', \"'s\", 'also', 'vast', 'collection', 'knowledge', ',', 'unhampered', 'mind', 'dream', 'sorts', 'uses', 'body', 'text', '.', 'What', 'build', 'corpus', 'set', 'English', 'Wikipedia', 'articles', ',', 'freely', 'conveniently', 'available', 'online', '.', 'In', 'order', 'easily', 'build', 'text', 'corpus', 'void', 'Wikipedia', 'article', 'markup', ',', 'use', 'gensim', ',', 'topic', 'modeling', 'library', 'Python', '.', 'A', 'Wikipedia', 'dump', 'file', 'also', 'required', 'procedure', ',', 'quite', 'obviously', '.', 'The', 'latest', 'files', 'found', '.', 'I', 'wrote', 'simple', 'Python', 'script', 'build', 'corpus', 'stripping', 'Wikipedia', 'markup', 'articles', ',', 'using', 'gensim', '.', 'You', 'read', 'WikiCorpus', 'class', '.', 'A', 'second', 'script', 'checks', 'corpus', 'text', 'file', 'built', '.', 'Now', ',', 'keep', 'mind', 'large', 'Wikipedia', 'dump', 'file', 'resulted', 'large', 'corpus', 'file', '.', 'Given', 'enormous', 'size', ',', 'may', 'dificulty', 'reading', 'full', 'file', 'memory', 'one', 'time', '.', 'This', 'script', ',', ',', 'starts', 'reading', '50', 'lines', '-', 'equates', '50', 'full', 'articles', '-', 'text', 'file', 'outputting', 'terminal', ',', 'press', 'key', 'output', 'another', '50', ',', 'type', \"'STOP\", \"'\", 'quit', '.', 'If', 'stop', ',', 'script', 'proceeds', 'load', 'entire', 'file', 'memory', '.', 'Which', 'could', 'problem', '.', 'You', ',', 'however', ',', 'verify', 'text', 'batches', 'lines', ',', 'order', 'satisfy', 'curiousity', 'something', 'good', 'happened', 'result', 'running', 'first', 'script', '.', 'If', 'planning', 'working', 'large', 'text', 'file', ',', 'may', 'need', 'workarounds', 'large', 'size', 'comparison', 'machine', \"'s\", 'memory', '.']\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0bqtXRsxR9hd","executionInfo":{"status":"ok","timestamp":1637059815906,"user_tz":-360,"elapsed":565,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"606095a6-682a-49d8-9abf-5d97c81d1920"},"source":["punctuated_list = []\n","final_punctuated_list = []\n","for char in stopped_list:\n","  punctuated_list .append(re.sub('[%s]' % re.escape('''!#$%&'()*+,-./:;<=>?@[\\]^_`{|}~'''), '', char))\n","for c in punctuated_list:\n","  final_punctuated_list .append(re.sub('[%s]' % re.escape(\"\"\".';\"\"\"), '', c))\n","print(final_punctuated_list)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['One', 'first', 'things', 'required', 'natural', 'language', 'processing', '', 'NLP', '', 'tasks', 'corpus', '', 'In', 'linguistics', 'NLP', '', 'corpus', 'refers', 'collection', 'texts', '', 'Such', 'collections', 'may', 'formed', 'single', 'language', 'texts', '', 'span', 'multiple', 'languages', '', 'numerous', 'reasons', 'multilingual', 'corpora', '', 'plural', 'corpus', '', 'may', 'useful', '', 'Corpora', 'may', 'also', 'consist', 'themed', 'texts', '', 'like', 'historical', '', 'Biblical', '', 'etc', '', 'Corpora', 'generally', 'solely', 'used', 'statistical', 'linguistic', 'analysis', 'hypothesis', 'testing', '', 'The', 'good', 'thing', 'internet', 'filled', 'text', '', 'many', 'cases', 'text', 'collected', 'well', 'oganized', '', 'even', 'requires', 'finessing', 'usable', '', 'preciselydefined', 'format', '', 'Wikipedia', '', 'particular', '', 'rich', 'source', 'wellorganized', 'textual', 'data', '', 'It', 's', 'also', 'vast', 'collection', 'knowledge', '', 'unhampered', 'mind', 'dream', 'sorts', 'uses', 'body', 'text', '', 'What', 'build', 'corpus', 'set', 'English', 'Wikipedia', 'articles', '', 'freely', 'conveniently', 'available', 'online', '', 'In', 'order', 'easily', 'build', 'text', 'corpus', 'void', 'Wikipedia', 'article', 'markup', '', 'use', 'gensim', '', 'topic', 'modeling', 'library', 'Python', '', 'A', 'Wikipedia', 'dump', 'file', 'also', 'required', 'procedure', '', 'quite', 'obviously', '', 'The', 'latest', 'files', 'found', '', 'I', 'wrote', 'simple', 'Python', 'script', 'build', 'corpus', 'stripping', 'Wikipedia', 'markup', 'articles', '', 'using', 'gensim', '', 'You', 'read', 'WikiCorpus', 'class', '', 'A', 'second', 'script', 'checks', 'corpus', 'text', 'file', 'built', '', 'Now', '', 'keep', 'mind', 'large', 'Wikipedia', 'dump', 'file', 'resulted', 'large', 'corpus', 'file', '', 'Given', 'enormous', 'size', '', 'may', 'dificulty', 'reading', 'full', 'file', 'memory', 'one', 'time', '', 'This', 'script', '', '', 'starts', 'reading', '50', 'lines', '', 'equates', '50', 'full', 'articles', '', 'text', 'file', 'outputting', 'terminal', '', 'press', 'key', 'output', 'another', '50', '', 'type', 'STOP', '', 'quit', '', 'If', 'stop', '', 'script', 'proceeds', 'load', 'entire', 'file', 'memory', '', 'Which', 'could', 'problem', '', 'You', '', 'however', '', 'verify', 'text', 'batches', 'lines', '', 'order', 'satisfy', 'curiousity', 'something', 'good', 'happened', 'result', 'running', 'first', 'script', '', 'If', 'planning', 'working', 'large', 'text', 'file', '', 'may', 'need', 'workarounds', 'large', 'size', 'comparison', 'machine', 's', 'memory', '']\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9EUgAMRFNZNu","executionInfo":{"status":"ok","timestamp":1637058777050,"user_tz":-360,"elapsed":598,"user":{"displayName":"Abs Sayem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi123LiAL3lXTjZK_Zppo_vnwsE5vnd-_u9Kys9=s64","userId":"15293502892038986257"}},"outputId":"bcdaeb6e-9621-432e-97c6-28db138b94c0"},"source":["# PoS-Taging thye words--------\n","tagged_list = nltk.pos_tag(stopped_list)\n","tagged_list"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('One', 'CD'),\n"," ('first', 'JJ'),\n"," ('things', 'NNS'),\n"," ('required', 'VBN'),\n"," ('natural', 'JJ'),\n"," ('language', 'NN'),\n"," ('processing', 'NN'),\n"," ('(', '('),\n"," ('NLP', 'NNP'),\n"," (')', ')'),\n"," ('tasks', 'NNS'),\n"," ('corpus', 'NN'),\n"," ('.', '.'),\n"," ('In', 'IN'),\n"," ('linguistics', 'NNS'),\n"," ('NLP', 'NNP'),\n"," (',', ','),\n"," ('corpus', 'NN'),\n"," ('refers', 'NNS'),\n"," ('collection', 'VBP'),\n"," ('texts', 'RB'),\n"," ('.', '.'),\n"," ('Such', 'JJ'),\n"," ('collections', 'NNS'),\n"," ('may', 'MD'),\n"," ('formed', 'VB'),\n"," ('single', 'JJ'),\n"," ('language', 'NN'),\n"," ('texts', 'NN'),\n"," (',', ','),\n"," ('span', 'VBP'),\n"," ('multiple', 'JJ'),\n"," ('languages', 'NNS'),\n"," ('-', ':'),\n"," ('numerous', 'JJ'),\n"," ('reasons', 'NNS'),\n"," ('multilingual', 'JJ'),\n"," ('corpora', 'NN'),\n"," (',', ','),\n"," ('plural', 'JJ'),\n"," ('corpus', 'NN'),\n"," (',', ','),\n"," ('may', 'MD'),\n"," ('useful', 'VB'),\n"," ('.', '.'),\n"," ('Corpora', 'NNP'),\n"," ('may', 'MD'),\n"," ('also', 'RB'),\n"," ('consist', 'VB'),\n"," ('themed', 'JJ'),\n"," ('texts', 'NNS'),\n"," (',', ','),\n"," ('like-', 'JJ'),\n"," ('historical', 'JJ'),\n"," (',', ','),\n"," ('Biblical', 'NNP'),\n"," (',', ','),\n"," ('etc', 'FW'),\n"," ('.', '.'),\n"," ('Corpora', 'NNP'),\n"," ('generally', 'RB'),\n"," ('solely', 'RB'),\n"," ('used', 'JJ'),\n"," ('statistical', 'JJ'),\n"," ('linguistic', 'JJ'),\n"," ('analysis', 'NN'),\n"," ('hypothesis', 'NN'),\n"," ('testing', 'VBG'),\n"," ('.', '.'),\n"," ('The', 'DT'),\n"," ('good', 'JJ'),\n"," ('thing', 'NN'),\n"," ('internet', 'NN'),\n"," ('filled', 'VBN'),\n"," ('text', 'NN'),\n"," (',', ','),\n"," ('many', 'JJ'),\n"," ('cases', 'NNS'),\n"," ('text', 'RB'),\n"," ('collected', 'VBN'),\n"," ('well', 'RB'),\n"," ('oganized', 'VBN'),\n"," (',', ','),\n"," ('even', 'RB'),\n"," ('requires', 'VBZ'),\n"," ('finessing', 'VBG'),\n"," ('usable', 'JJ'),\n"," (',', ','),\n"," ('precisely-defined', 'JJ'),\n"," ('format', 'NN'),\n"," ('.', '.'),\n"," ('Wikipedia', 'NNP'),\n"," (',', ','),\n"," ('particular', 'JJ'),\n"," (',', ','),\n"," ('rich', 'JJ'),\n"," ('source', 'NN'),\n"," ('well-organized', 'JJ'),\n"," ('textual', 'JJ'),\n"," ('data', 'NN'),\n"," ('.', '.'),\n"," ('It', 'PRP'),\n"," (\"'s\", 'VBZ'),\n"," ('also', 'RB'),\n"," ('vast', 'JJ'),\n"," ('collection', 'NN'),\n"," ('knowledge', 'NN'),\n"," (',', ','),\n"," ('unhampered', 'JJ'),\n"," ('mind', 'NN'),\n"," ('dream', 'NN'),\n"," ('sorts', 'VBZ'),\n"," ('uses', 'VBZ'),\n"," ('body', 'NN'),\n"," ('text', 'NN'),\n"," ('.', '.'),\n"," ('What', 'WP'),\n"," ('build', 'VBD'),\n"," ('corpus', 'NN'),\n"," ('set', 'VBN'),\n"," ('English', 'NNP'),\n"," ('Wikipedia', 'NNP'),\n"," ('articles', 'NNS'),\n"," (',', ','),\n"," ('freely', 'RB'),\n"," ('conveniently', 'RB'),\n"," ('available', 'JJ'),\n"," ('online', 'NN'),\n"," ('.', '.'),\n"," ('In', 'IN'),\n"," ('order', 'NN'),\n"," ('easily', 'RB'),\n"," ('build', 'VB'),\n"," ('text', 'JJ'),\n"," ('corpus', 'NN'),\n"," ('void', 'NN'),\n"," ('Wikipedia', 'NNP'),\n"," ('article', 'NN'),\n"," ('markup', 'NN'),\n"," (',', ','),\n"," ('use', 'NN'),\n"," ('gensim', 'NN'),\n"," (',', ','),\n"," ('topic', 'NN'),\n"," ('modeling', 'VBG'),\n"," ('library', 'JJ'),\n"," ('Python', 'NNP'),\n"," ('.', '.'),\n"," ('A', 'DT'),\n"," ('Wikipedia', 'NNP'),\n"," ('dump', 'NN'),\n"," ('file', 'NN'),\n"," ('also', 'RB'),\n"," ('required', 'JJ'),\n"," ('procedure', 'NN'),\n"," (',', ','),\n"," ('quite', 'RB'),\n"," ('obviously', 'RB'),\n"," ('.', '.'),\n"," ('The', 'DT'),\n"," ('latest', 'JJS'),\n"," ('files', 'NNS'),\n"," ('found', 'VBD'),\n"," ('.', '.'),\n"," ('I', 'PRP'),\n"," ('wrote', 'VBD'),\n"," ('simple', 'JJ'),\n"," ('Python', 'NNP'),\n"," ('script', 'NN'),\n"," ('build', 'NN'),\n"," ('corpus', 'NN'),\n"," ('stripping', 'VBG'),\n"," ('Wikipedia', 'NNP'),\n"," ('markup', 'NN'),\n"," ('articles', 'NNS'),\n"," (',', ','),\n"," ('using', 'VBG'),\n"," ('gensim', 'NN'),\n"," ('.', '.'),\n"," ('You', 'PRP'),\n"," ('read', 'VBP'),\n"," ('WikiCorpus', 'JJ'),\n"," ('class', 'NN'),\n"," ('.', '.'),\n"," ('A', 'DT'),\n"," ('second', 'JJ'),\n"," ('script', 'NN'),\n"," ('checks', 'NNS'),\n"," ('corpus', 'VBP'),\n"," ('text', 'NN'),\n"," ('file', 'NN'),\n"," ('built', 'VBN'),\n"," ('.', '.'),\n"," ('Now', 'RB'),\n"," (',', ','),\n"," ('keep', 'VB'),\n"," ('mind', 'NN'),\n"," ('large', 'JJ'),\n"," ('Wikipedia', 'NNP'),\n"," ('dump', 'NN'),\n"," ('file', 'NN'),\n"," ('resulted', 'VBD'),\n"," ('large', 'JJ'),\n"," ('corpus', 'NN'),\n"," ('file', 'NN'),\n"," ('.', '.'),\n"," ('Given', 'NNP'),\n"," ('enormous', 'JJ'),\n"," ('size', 'NN'),\n"," (',', ','),\n"," ('may', 'MD'),\n"," ('dificulty', 'VB'),\n"," ('reading', 'VBG'),\n"," ('full', 'JJ'),\n"," ('file', 'NN'),\n"," ('memory', 'NN'),\n"," ('one', 'CD'),\n"," ('time', 'NN'),\n"," ('.', '.'),\n"," ('This', 'DT'),\n"," ('script', 'NN'),\n"," (',', ','),\n"," (',', ','),\n"," ('starts', 'VBZ'),\n"," ('reading', 'VBG'),\n"," ('50', 'CD'),\n"," ('lines', 'NNS'),\n"," ('-', ':'),\n"," ('equates', 'VBZ'),\n"," ('50', 'CD'),\n"," ('full', 'JJ'),\n"," ('articles', 'NNS'),\n"," ('-', ':'),\n"," ('text', 'NN'),\n"," ('file', 'NN'),\n"," ('outputting', 'VBG'),\n"," ('terminal', 'JJ'),\n"," (',', ','),\n"," ('press', 'JJ'),\n"," ('key', 'NN'),\n"," ('output', 'NN'),\n"," ('another', 'DT'),\n"," ('50', 'CD'),\n"," (',', ','),\n"," ('type', 'NN'),\n"," (\"'STOP\", 'POS'),\n"," (\"'\", 'POS'),\n"," ('quit', 'NN'),\n"," ('.', '.'),\n"," ('If', 'IN'),\n"," ('stop', 'JJ'),\n"," (',', ','),\n"," ('script', 'JJ'),\n"," ('proceeds', 'NNS'),\n"," ('load', 'NN'),\n"," ('entire', 'JJ'),\n"," ('file', 'NN'),\n"," ('memory', 'NN'),\n"," ('.', '.'),\n"," ('Which', 'NNP'),\n"," ('could', 'MD'),\n"," ('problem', 'NN'),\n"," ('.', '.'),\n"," ('You', 'PRP'),\n"," (',', ','),\n"," ('however', 'RB'),\n"," (',', ','),\n"," ('verify', 'VBP'),\n"," ('text', 'JJ'),\n"," ('batches', 'NNS'),\n"," ('lines', 'NNS'),\n"," (',', ','),\n"," ('order', 'NN'),\n"," ('satisfy', 'NNS'),\n"," ('curiousity', 'NN'),\n"," ('something', 'NN'),\n"," ('good', 'JJ'),\n"," ('happened', 'VBD'),\n"," ('result', 'NN'),\n"," ('running', 'VBG'),\n"," ('first', 'JJ'),\n"," ('script', 'NN'),\n"," ('.', '.'),\n"," ('If', 'IN'),\n"," ('planning', 'VBG'),\n"," ('working', 'VBG'),\n"," ('large', 'JJ'),\n"," ('text', 'NN'),\n"," ('file', 'NN'),\n"," (',', ','),\n"," ('may', 'MD'),\n"," ('need', 'VB'),\n"," ('workarounds', 'NNS'),\n"," ('large', 'JJ'),\n"," ('size', 'NN'),\n"," ('comparison', 'NN'),\n"," ('machine', 'NN'),\n"," (\"'s\", 'POS'),\n"," ('memory', 'NN'),\n"," ('.', '.')]"]},"metadata":{},"execution_count":59}]}]}